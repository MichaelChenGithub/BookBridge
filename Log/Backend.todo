☐ **[資料前處理]**
    - **工具**: Apache Spark + Local Jupyter notebook + Google Cloud Dataproc
    - **任務**:
        1. 讀取 `Amazon-Reviews-2023 (Books)` 原始資料 (JSON/CSV)。
        2. **資料縮減**: 篩選並保留互動量最高的 Top 100,000 本書。
        3. **資料轉換**: 提取 `(user_id, book_id, timestamp)` 互動資料。
        4. **序列生成**: 為每一位使用者，按時間排序生成互動序列（例如：`[book_id_A, book_id_B, book_id_C, ...]`）。
        5. 將處理後的序列資料儲存為 Parquet 格式，以供 `Gensim` 讀取。
☐ **[模型訓練]**
    - **工具**: `Gensim` (Word2Vec 模組) + Local Jupyter notebook + GCE
    - **執行環境**: GCE (Google Compute Engine) VM（作為一次性任務執行，非 24/7 服務）。
    - **任務**: 在 Spark 處理完的序列資料上訓練 `item2vec` (Word2Vec) 模型。
    - **設定**: `dim=256` (或其他您選擇的維度)。
☐ **[資產產出]**
    ☐ **任務**: 訓練完成後，匯出 API 服務所需的三個關鍵檔案：
        1. `embeddings.npy`: 一個 `numpy` 陣列檔案，儲存 100k 本書的嵌入向量。
            - *維度*: `(100000, 256)`
            - *大小估算*: 100,000 * 256 * 4 bytes ≈ 100 MB
        2. `mapping.json`: 儲存 `title` 和 `index` 之間的雙向映射。JSON
            
            `{
              "title_to_index": {"哈利波特": 0, "魔戒": 1, ...},
              "index_to_title": ["哈利波特", "魔戒", ...]
            }`
            
        3. `book_details.json`: 一個字典，儲存 100k 本書的詳細資訊（作者、摘要），以便在 API 回應時能快速查閱。JSON
            
            `{
              "哈利波特": {"author": "J.K. Rowling", "summary": "..."},
              "魔戒": {"author": "J.R.R. Tolkien", "summary": "..."}
            }`
            
    ☐ **儲存**: 將這三個產出的檔案上傳至 **GCS (Google Cloud Storage)**。