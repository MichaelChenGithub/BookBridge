{"cells":[{"cell_type":"code","execution_count":null,"id":"ba4dc663","metadata":{},"outputs":[],"source":["# BookBridge ETL for Dataproc\n","\n","# This script is designed to run on a GCP Dataproc cluster as a Spark job.\n","# It does two main things:\n","\n","# 1. Review ETL:\n","#    - Load Amazon reviews JSONL from GCS\n","#    - Filter verified purchases\n","#    - Select top N popular books by asin\n","#    - Build user purchase sequences\n","#    - Export item2vec training corpus as gzipped text (one sequence per line)\n","\n","# 2. Metadata ETL:\n","#    - Load book metadata JSONL from GCS\n","#    - Select a slim set of fields useful for recommendation display\n","#    - Join with top books\n","#    - Export as JSON (for API / Cloud Run to consume)\n","\n","# Usage (Dataproc / spark-submit):\n","\n","#   spark-submit \\\n","#     --deploy-mode cluster \\\n","#     BookBridgeETL.py \\\n","#     --bucket book_bridge \\\n","#     --top-k 100000"]},{"cell_type":"code","execution_count":2,"id":"936501f5","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Setting default log level to \"WARN\".\n","To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n","25/11/26 20:05:36 INFO SparkEnv: Registering MapOutputTracker\n","25/11/26 20:05:36 INFO SparkEnv: Registering BlockManagerMaster\n","25/11/26 20:05:36 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n","25/11/26 20:05:37 INFO SparkEnv: Registering OutputCommitCoordinator\n"]}],"source":["from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder \\\n","    .appName(\"BookBridge_ETL\") \\\n","    .getOrCreate()\n","\n","print(f\"Spark Version: {spark.version}\")\n","print(f\"App Name: {spark.sparkContext.appName}\")"]},{"cell_type":"code","execution_count":6,"id":"77046a69","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Loading data from: gs://book_bridge/Books.jsonl\n"]}],"source":["BUCKET_NAME = \"book_bridge\"\n","\n","# Define paths to books files in GCS\n","reviews_path = f\"gs://{BUCKET_NAME}/Books.jsonl\"\n","meta_path = f\"gs://{BUCKET_NAME}/meta_Books.jsonl\"\n","\n","print(f\"Loading data from: {reviews_path}\")"]},{"cell_type":"code","execution_count":8,"id":"0e84a281","metadata":{"scrolled":false},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["root\n"," |-- author: struct (nullable = true)\n"," |    |-- about: array (nullable = true)\n"," |    |    |-- element: string (containsNull = true)\n"," |    |-- avatar: string (nullable = true)\n"," |    |-- name: string (nullable = true)\n"," |-- average_rating: double (nullable = true)\n"," |-- bought_together: string (nullable = true)\n"," |-- categories: array (nullable = true)\n"," |    |-- element: string (containsNull = true)\n"," |-- description: array (nullable = true)\n"," |    |-- element: string (containsNull = true)\n"," |-- details: struct (nullable = true)\n"," |    |-- 11 54: string (nullable = true)\n"," |    |-- 15 69: string (nullable = true)\n"," |    |-- 18 86: string (nullable = true)\n"," |    |-- 22 49: string (nullable = true)\n"," |    |-- 3 5 and 5 25 disks: string (nullable = true)\n"," |    |-- 3 5 disk: string (nullable = true)\n"," |    |-- Accessory: string (nullable = true)\n"," |    |-- Additional product features: string (nullable = true)\n"," |    |-- Address Book: string (nullable = true)\n"," |    |-- Age Range (Description): string (nullable = true)\n"," |    |-- Album: string (nullable = true)\n"," |    |-- Amazon Listbooks Code: string (nullable = true)\n"," |    |-- Apparel: string (nullable = true)\n"," |    |-- Are Batteries Included: string (nullable = true)\n"," |    |-- Assembly Required: string (nullable = true)\n"," |    |-- Assembly required: string (nullable = true)\n"," |    |-- Audio CD: string (nullable = true)\n"," |    |-- Audio CD Library Binding: string (nullable = true)\n"," |    |-- Audio Cassette: string (nullable = true)\n"," |    |-- Automotive: string (nullable = true)\n"," |    |-- Average Battery Life: string (nullable = true)\n"," |    |-- Baby Product: string (nullable = true)\n"," |    |-- Back Material: string (nullable = true)\n"," |    |-- Back Material Type: string (nullable = true)\n"," |    |-- Backing: string (nullable = true)\n"," |    |-- Bath Book: string (nullable = true)\n"," |    |-- Batteries: string (nullable = true)\n"," |    |-- Batteries Included?: string (nullable = true)\n"," |    |-- Batteries Required?: string (nullable = true)\n"," |    |-- Batteries required: string (nullable = true)\n"," |    |-- Battery Cell Type: string (nullable = true)\n"," |    |-- Battery Description: string (nullable = true)\n"," |    |-- Best Sellers Rank: struct (nullable = true)\n"," |    |    |-- Adventure Travel: long (nullable = true)\n"," |    |    |-- Alphabet Reference: long (nullable = true)\n"," |    |    |-- Animal Calendars: long (nullable = true)\n"," |    |    |-- Antarctica Travel Guides: long (nullable = true)\n"," |    |    |-- Antiques & Collectibles: long (nullable = true)\n"," |    |    |-- Armenia Travel Guides: long (nullable = true)\n"," |    |    |-- Arts & Photography: long (nullable = true)\n"," |    |    |-- Arts, Crafts & Sewing: long (nullable = true)\n"," |    |    |-- Asian Georgia Travel Guides: long (nullable = true)\n"," |    |    |-- Asian Myth & Legend: long (nullable = true)\n"," |    |    |-- Astrophysics & Space Science: long (nullable = true)\n"," |    |    |-- Atlases: long (nullable = true)\n"," |    |    |-- Atlases & Maps: long (nullable = true)\n"," |    |    |-- Automotive: long (nullable = true)\n"," |    |    |-- Automotive Engineering: long (nullable = true)\n"," |    |    |-- Automotive Repair: long (nullable = true)\n"," |    |    |-- Banjo Songbooks: long (nullable = true)\n"," |    |    |-- Banjos: long (nullable = true)\n"," |    |    |-- Banks & Banking: long (nullable = true)\n"," |    |    |-- Barcelona Travel Guides: long (nullable = true)\n"," |    |    |-- Bird Field Guides: long (nullable = true)\n"," |    |    |-- Blank Sheet Music: long (nullable = true)\n"," |    |    |-- Board Games: long (nullable = true)\n"," |    |    |-- Book Making & Binding: long (nullable = true)\n"," |    |    |-- Books: long (nullable = true)\n"," |    |    |-- Books on CD: long (nullable = true)\n"," |    |    |-- Boston Massachusetts Travel Books: long (nullable = true)\n"," |    |    |-- Brass Songbooks: long (nullable = true)\n"," |    |    |-- British & Irish Poetry: long (nullable = true)\n"," |    |    |-- Budgeting & Money Management: long (nullable = true)\n"," |    |    |-- Bulletin Boards: long (nullable = true)\n"," |    |    |-- Business & Money: long (nullable = true)\n"," |    |    |-- Cake Baking: long (nullable = true)\n"," |    |    |-- Calculus: long (nullable = true)\n"," |    |    |-- Calendars: long (nullable = true)\n"," |    |    |-- Calendars, Planners & Organizers: long (nullable = true)\n"," |    |    |-- Call of Cthulhu Game: long (nullable = true)\n"," |    |    |-- Cambodia Travel Guides: long (nullable = true)\n"," |    |    |-- Camping: long (nullable = true)\n"," |    |    |-- Canning & Preserving: long (nullable = true)\n"," |    |    |-- Card Games: long (nullable = true)\n"," |    |    |-- Cardiovascular Diseases: long (nullable = true)\n"," |    |    |-- Cards & Card Stock: long (nullable = true)\n"," |    |    |-- Cast Iron Recipes: long (nullable = true)\n"," |    |    |-- Cat Calendars: long (nullable = true)\n"," |    |    |-- Cell Phone Stands: long (nullable = true)\n"," |    |    |-- Cell Phones & Accessories: long (nullable = true)\n"," |    |    |-- Cello Songbooks: long (nullable = true)\n"," |    |    |-- Cellos: long (nullable = true)\n"," |    |    |-- Chess: long (nullable = true)\n"," |    |    |-- Children & Teens Christian Education: long (nullable = true)\n"," |    |    |-- Children's Action & Adventure Books: long (nullable = true)\n"," |    |    |-- Children's Activities, Crafts & Games Books: long (nullable = true)\n"," |    |    |-- Children's Activity Books: long (nullable = true)\n"," |    |    |-- Children's American Revolution History: long (nullable = true)\n"," |    |    |-- Children's Animals Books: long (nullable = true)\n"," |    |    |-- Children's Arithmetic Books: long (nullable = true)\n"," |    |    |-- Children's Art Books: long (nullable = true)\n"," |    |    |-- Children's Art Fiction: long (nullable = true)\n"," |    |    |-- Children's Arts, Music & Photography Books: long (nullable = true)\n"," |    |    |-- Children's Beadwork, Fashion & Jewelry Craft Books: long (nullable = true)\n"," |    |    |-- Children's Bear Books: long (nullable = true)\n"," |    |    |-- Children's Beginner Readers: long (nullable = true)\n"," |    |    |-- Children's Board Games Books: long (nullable = true)\n"," |    |    |-- Children's Books: long (nullable = true)\n"," |    |    |-- Children's Books on Emotions & Feelings: long (nullable = true)\n"," |    |    |-- Children's Card Games Books: long (nullable = true)\n"," |    |    |-- Children's Cars & Trucks Books: long (nullable = true)\n"," |    |    |-- Children's Chapter Books: long (nullable = true)\n"," |    |    |-- Children's Christian Fiction Books: long (nullable = true)\n"," |    |    |-- Children's Classics: long (nullable = true)\n"," |    |    |-- Children's Coloring Books: long (nullable = true)\n"," |    |    |-- Children's Composition & Creative Writing Books: long (nullable = true)\n"," |    |    |-- Children's Cookbooks: long (nullable = true)\n"," |    |    |-- Children's Craft & Hobby Books: long (nullable = true)\n"," |    |    |-- Children's Dot to Dot Activity Books: long (nullable = true)\n"," |    |    |-- Children's Dragon, Unicorn & Mythical Stories: long (nullable = true)\n"," |    |    |-- Children's Drawing Books: long (nullable = true)\n"," |    |    |-- Children's ESL Books: long (nullable = true)\n"," |    |    |-- Children's Europe Books: long (nullable = true)\n"," |    |    |-- Children's European History: long (nullable = true)\n"," |    |    |-- Children's Family Life Books: long (nullable = true)\n"," |    |    |-- Children's Fashion Books: long (nullable = true)\n"," |    |    |-- Children's Flower & Plant Books: long (nullable = true)\n"," |    |    |-- Children's Foreign Language Books: long (nullable = true)\n"," |    |    |-- Children's Friendship Books: long (nullable = true)\n"," |    |    |-- Children's Game Books: long (nullable = true)\n"," |    |    |-- Children's General Humor Books: long (nullable = true)\n"," |    |    |-- Children's General Social Science Books: long (nullable = true)\n"," |    |    |-- Children's General Study Aid Books: long (nullable = true)\n"," |    |    |-- Children's Grammar Books: long (nullable = true)\n"," |    |    |-- Children's Hispanic & Latino Books: long (nullable = true)\n"," |    |    |-- Children's Humor: long (nullable = true)\n"," |    |    |-- Children's Interactive Adventures: long (nullable = true)\n"," |    |    |-- Children's Literature: long (nullable = true)\n"," |    |    |-- Children's Math Books: long (nullable = true)\n"," |    |    |-- Children's Maze Books: long (nullable = true)\n"," |    |    |-- Children's Military Books: long (nullable = true)\n"," |    |    |-- Children's Multicultural Biographies: long (nullable = true)\n"," |    |    |-- Children's Music: long (nullable = true)\n"," |    |    |-- Children's Music Books: long (nullable = true)\n"," |    |    |-- Children's Oceanography Books: long (nullable = true)\n"," |    |    |-- Children's Papercrafts Books: long (nullable = true)\n"," |    |    |-- Children's Photography Books: long (nullable = true)\n"," |    |    |-- Children's Puzzle Books: long (nullable = true)\n"," |    |    |-- Children's Questions & Answer Game Books: long (nullable = true)\n"," |    |    |-- Children's Reading & Writing Education Books: long (nullable = true)\n"," |    |    |-- Children's Reference & Nonfiction: long (nullable = true)\n"," |    |    |-- Children's School Issues: long (nullable = true)\n"," |    |    |-- Children's Science & Nature Books: long (nullable = true)\n"," |    |    |-- Children's Science Experiment Books: long (nullable = true)\n"," |    |    |-- Children's Sculpture Books: long (nullable = true)\n"," |    |    |-- Children's Self-Esteem Books: long (nullable = true)\n"," |    |    |-- Children's Sense & Sensation Books: long (nullable = true)\n"," |    |    |-- Children's Size & Shape Books: long (nullable = true)\n"," |    |    |-- Children's Sports & Outdoors Books: long (nullable = true)\n"," |    |    |-- Children's Superhero Fiction: long (nullable = true)\n"," |    |    |-- Children's Test Preparation Books: long (nullable = true)\n"," |    |    |-- Children's Transportation Books: long (nullable = true)\n"," |    |    |-- Children's Values Books: long (nullable = true)\n"," |    |    |-- Children's Vocabulary & Spelling Books: long (nullable = true)\n"," |    |    |-- Children's Water Books: long (nullable = true)\n"," |    |    |-- Children's Word Games Books: long (nullable = true)\n"," |    |    |-- Choral Songbooks: long (nullable = true)\n"," |    |    |-- Christian Bibles: long (nullable = true)\n"," |    |    |-- Christian Books & Bibles: long (nullable = true)\n"," |    |    |-- Christian Counseling: long (nullable = true)\n"," |    |    |-- Christian Devotionals: long (nullable = true)\n"," |    |    |-- Christian Inspirational: long (nullable = true)\n"," |    |    |-- Christian Meditation Worship & Devotion: long (nullable = true)\n"," |    |    |-- Christian New Testament References: long (nullable = true)\n"," |    |    |-- Christian Personal Growth: long (nullable = true)\n"," |    |    |-- Christian Saints: long (nullable = true)\n"," |    |    |-- Christian Spiritual Growth: long (nullable = true)\n"," |    |    |-- Christmas: long (nullable = true)\n"," |    |    |-- Citizenship Test Guides: long (nullable = true)\n"," |    |    |-- Civil & Environmental Engineering: long (nullable = true)\n"," |    |    |-- Clarinet Songbooks: long (nullable = true)\n"," |    |    |-- Clarinets: long (nullable = true)\n"," |    |    |-- Classic Literature & Fiction: long (nullable = true)\n"," |    |    |-- Coloring Books for Grown-Ups: long (nullable = true)\n"," |    |    |-- Comics & Graphic Novels: long (nullable = true)\n"," |    |    |-- Comics & Manga Coloring Books for Grown-Ups: long (nullable = true)\n"," |    |    |-- Common Core: long (nullable = true)\n"," |    |    |-- Communications: long (nullable = true)\n"," |    |    |-- Computer & Video Game Strategy Guides: long (nullable = true)\n"," |    |    |-- Computer Software: long (nullable = true)\n"," |    |    |-- Consumer Guides: long (nullable = true)\n"," |    |    |-- Contemporary Literature & Fiction: long (nullable = true)\n"," |    |    |-- Contemporary Romance: long (nullable = true)\n"," |    |    |-- Cookbooks, Food & Wine: long (nullable = true)\n"," |    |    |-- Cooking Humor: long (nullable = true)\n"," |    |    |-- Costa Rica Travel Guides: long (nullable = true)\n"," |    |    |-- Counting & Numeration: long (nullable = true)\n"," |    |    |-- Crafts & Hobbies: long (nullable = true)\n"," |    |    |-- Cross-Country Skiing: long (nullable = true)\n"," |    |    |-- Cross-Stitch: long (nullable = true)\n"," |    |    |-- Cross-Stitch Supplies: long (nullable = true)\n"," |    |    |-- Cultural Anthropology: long (nullable = true)\n"," |    |    |-- Curricula: long (nullable = true)\n"," |    |    |-- Cycling Travel Guides: long (nullable = true)\n"," |    |    |-- Cyprus Travel Guides: long (nullable = true)\n"," |    |    |-- Dark Fantasy: long (nullable = true)\n"," |    |    |-- Deals: long (nullable = true)\n"," |    |    |-- Dessert Baking: long (nullable = true)\n"," |    |    |-- Dictionaries & Thesauruses: long (nullable = true)\n"," |    |    |-- Do-It-Yourself Home Improvement: long (nullable = true)\n"," |    |    |-- Dog Calendars: long (nullable = true)\n"," |    |    |-- Dog Care: long (nullable = true)\n"," |    |    |-- Dragons & Mythical Creatures Fantasy: long (nullable = true)\n"," |    |    |-- Drawing: long (nullable = true)\n"," |    |    |-- Dungeons & Dragons Game: long (nullable = true)\n"," |    |    |-- Early Childhood Education: long (nullable = true)\n"," |    |    |-- Early Childhood Education Materials: long (nullable = true)\n"," |    |    |-- Early Development & Activity Toys: long (nullable = true)\n"," |    |    |-- Economics: long (nullable = true)\n"," |    |    |-- Education: long (nullable = true)\n"," |    |    |-- Education & Teaching: long (nullable = true)\n"," |    |    |-- Education Assessment: long (nullable = true)\n"," |    |    |-- Education Curriculum & Instruction: long (nullable = true)\n"," |    |    |-- Education Funding: long (nullable = true)\n"," |    |    |-- Education Supplies & Craft Supplies: long (nullable = true)\n"," |    |    |-- Education Theory: long (nullable = true)\n"," |    |    |-- Education Workbooks: long (nullable = true)\n"," |    |    |-- Educational Certification & Development: long (nullable = true)\n"," |    |    |-- Electric Machinery & Motors: long (nullable = true)\n"," |    |    |-- Elementary Education: long (nullable = true)\n"," |    |    |-- Emergency Medical Services: long (nullable = true)\n"," |    |    |-- Emigration & Immigration Studies: long (nullable = true)\n"," |    |    |-- English, Scottish & Welsh Cooking & Wine: long (nullable = true)\n"," |    |    |-- Entertaining & Holiday Cooking: long (nullable = true)\n"," |    |    |-- Epic Fantasy: long (nullable = true)\n"," |    |    |-- Etymology: long (nullable = true)\n"," |    |    |-- European History: long (nullable = true)\n"," |    |    |-- Exercise & Fitness Injury Prevention: long (nullable = true)\n"," |    |    |-- Expeditions & Discoveries World History: long (nullable = true)\n"," |    |    |-- Family Conflict Resolution: long (nullable = true)\n"," |    |    |-- Family Health: long (nullable = true)\n"," |    |    |-- Family Practice Medicine: long (nullable = true)\n"," |    |    |-- Fantasy Gaming: long (nullable = true)\n"," |    |    |-- Fiction Writing Reference: long (nullable = true)\n"," |    |    |-- Field Guides: long (nullable = true)\n"," |    |    |-- Finland Travel Guides: long (nullable = true)\n"," |    |    |-- Flower Arranging: long (nullable = true)\n"," |    |    |-- Flowers & Landscapes Coloring Books for Grown-Ups: long (nullable = true)\n"," |    |    |-- Flute Songbooks: long (nullable = true)\n"," |    |    |-- Flutes: long (nullable = true)\n"," |    |    |-- Folk & Traditional Songbooks: long (nullable = true)\n"," |    |    |-- Folklore: long (nullable = true)\n"," |    |    |-- Foreign Language Calendars: long (nullable = true)\n"," |    |    |-- Forests & Rainforests: long (nullable = true)\n"," |    |    |-- French Horn Songbooks: long (nullable = true)\n"," |    |    |-- Frozen Dessert Recipes: long (nullable = true)\n"," |    |    |-- Game Calendars: long (nullable = true)\n"," |    |    |-- General Argentina Travel Guides: long (nullable = true)\n"," |    |    |-- General Austria Travel Guides: long (nullable = true)\n"," |    |    |-- General Belarus & Ukraine Travel Guides: long (nullable = true)\n"," |    |    |-- General Brazil Travel Guides: long (nullable = true)\n"," |    |    |-- General Chemistry: long (nullable = true)\n"," |    |    |-- General Cuba Travel Guides: long (nullable = true)\n"," |    |    |-- General France Travel Guides: long (nullable = true)\n"," |    |    |-- General Great Britain Travel Guides: long (nullable = true)\n"," |    |    |-- General Hungary Travel Guides: long (nullable = true)\n"," |    |    |-- General Ireland Travel Guides: long (nullable = true)\n"," |    |    |-- General Netherlands Travel Guides: long (nullable = true)\n"," |    |    |-- General Poland Travel Guides: long (nullable = true)\n"," |    |    |-- General Scotland Travel Guides: long (nullable = true)\n"," |    |    |-- General Spain Travel Guides: long (nullable = true)\n"," |    |    |-- General Travel Reference: long (nullable = true)\n"," |    |    |-- General Turkey Travel Guides: long (nullable = true)\n"," |    |    |-- Genetics: long (nullable = true)\n"," |    |    |-- Genre Literature & Fiction: long (nullable = true)\n"," |    |    |-- German Travel Guides: long (nullable = true)\n"," |    |    |-- Gospel Music: long (nullable = true)\n"," |    |    |-- Grammar Reference: long (nullable = true)\n"," |    |    |-- Guitar Songbooks: long (nullable = true)\n"," |    |    |-- Guitars: long (nullable = true)\n"," |    |    |-- Happiness Self-Help: long (nullable = true)\n"," |    |    |-- Harmonica Songbooks: long (nullable = true)\n"," |    |    |-- Harmonicas: long (nullable = true)\n"," |    |    |-- Health Teaching Materials: long (nullable = true)\n"," |    |    |-- Health, Fitness & Dieting: long (nullable = true)\n"," |    |    |-- Heat Press Parts & Accessories: long (nullable = true)\n"," |    |    |-- Hiking & Camping Excursion Guides: long (nullable = true)\n"," |    |    |-- Hiking & Camping Instructional Guides: long (nullable = true)\n"," |    |    |-- Historical Fiction Manga: long (nullable = true)\n"," |    |    |-- Holidays: long (nullable = true)\n"," |    |    |-- Home-Based Businesses: long (nullable = true)\n"," |    |    |-- Homeschooling: long (nullable = true)\n"," |    |    |-- Honduras Travel Guides: long (nullable = true)\n"," |    |    |-- Horns: long (nullable = true)\n"," |    |    |-- Horse Calendars: long (nullable = true)\n"," |    |    |-- Humor: long (nullable = true)\n"," |    |    |-- Humor & Comic Calendars: long (nullable = true)\n"," |    |    |-- Humor & Entertainment: long (nullable = true)\n"," |    |    |-- Hunting: long (nullable = true)\n"," |    |    |-- Iditarod & Dog-Sledding: long (nullable = true)\n"," |    |    |-- Industrial Health & Safety: long (nullable = true)\n"," |    |    |-- Industrial Manufacturing Systems: long (nullable = true)\n"," |    |    |-- Inspiration & Spirituality: long (nullable = true)\n"," |    |    |-- Instruction Methods: long (nullable = true)\n"," |    |    |-- Introductory & Beginning Programming: long (nullable = true)\n"," |    |    |-- Investing: long (nullable = true)\n"," |    |    |-- Iraq Travel Guides: long (nullable = true)\n"," |    |    |-- Italian Cooking, Food & Wine: long (nullable = true)\n"," |    |    |-- Jazz Music: long (nullable = true)\n"," |    |    |-- Jazz Songbooks: long (nullable = true)\n"," |    |    |-- Job Hunting & Career Guides: long (nullable = true)\n"," |    |    |-- Journal Writing Self-Help: long (nullable = true)\n"," |    |    |-- Journalism Writing Reference: long (nullable = true)\n"," |    |    |-- Kitchen Appliance Cooking: long (nullable = true)\n"," |    |    |-- Knitting: long (nullable = true)\n"," |    |    |-- Knitting & Crochet Supplies: long (nullable = true)\n"," |    |    |-- Kosher Cooking: long (nullable = true)\n"," |    |    |-- Labor & Industrial Economic Relations: long (nullable = true)\n"," |    |    |-- Labor & Industrial Relations: long (nullable = true)\n"," |    |    |-- Landscape & Seascape Art: long (nullable = true)\n"," |    |    |-- Landscape Painting: long (nullable = true)\n"," |    |    |-- Landscape Photography: long (nullable = true)\n"," |    |    |-- Language Arts Teaching Materials: long (nullable = true)\n"," |    |    |-- Laos Travel Guides: long (nullable = true)\n"," |    |    |-- Law: long (nullable = true)\n"," |    |    |-- Libros en espa√±ol: long (nullable = true)\n"," |    |    |-- Lisbon Travel Guides: long (nullable = true)\n"," |    |    |-- Literary Fiction: long (nullable = true)\n"," |    |    |-- Logic & Brain Teasers: long (nullable = true)\n"," |    |    |-- Low Fat Diets: long (nullable = true)\n"," |    |    |-- Mandalas & Patterns Coloring Books for Grown-Ups: long (nullable = true)\n"," |    |    |-- Mandolin Songbooks: long (nullable = true)\n"," |    |    |-- Mandolins: long (nullable = true)\n"," |    |    |-- Maps: long (nullable = true)\n"," |    |    |-- Marvel Comics & Graphic Novels: long (nullable = true)\n"," |    |    |-- Math Teaching Materials: long (nullable = true)\n"," |    |    |-- Mathematics: long (nullable = true)\n"," |    |    |-- Mathematics Study & Teaching: long (nullable = true)\n"," |    |    |-- Medical Anatomy: long (nullable = true)\n"," |    |    |-- Medical Books: long (nullable = true)\n"," |    |    |-- Medical Instruments & Supplies: long (nullable = true)\n"," |    |    |-- Medical School Guides: long (nullable = true)\n"," |    |    |-- Memoirs: long (nullable = true)\n"," |    |    |-- Mexico History: long (nullable = true)\n"," |    |    |-- Mid Atlantic US Travel Books: long (nullable = true)\n"," |    |    |-- Model Building: long (nullable = true)\n"," |    |    |-- Mortgages: long (nullable = true)\n"," |    |    |-- Motorcycle Repair & Performance: long (nullable = true)\n"," |    |    |-- Motorcycles: long (nullable = true)\n"," |    |    |-- Mountain Climbing: long (nullable = true)\n"," |    |    |-- Movie Guides & Reviews: long (nullable = true)\n"," |    |    |-- Music: long (nullable = true)\n"," |    |    |-- Music Exercises: long (nullable = true)\n"," |    |    |-- Music Instruction & Study: long (nullable = true)\n"," |    |    |-- Music Reference: long (nullable = true)\n"," |    |    |-- Music Techniques: long (nullable = true)\n"," |    |    |-- Music Theory: long (nullable = true)\n"," |    |    |-- Music Theory, Composition & Performance: long (nullable = true)\n"," |    |    |-- Musical Instruments: long (nullable = true)\n"," |    |    |-- Musicals & Film Songbooks: long (nullable = true)\n"," |    |    |-- Mystery Graphic Novels: long (nullable = true)\n"," |    |    |-- Nature & Ecology: long (nullable = true)\n"," |    |    |-- Nature Calendars: long (nullable = true)\n"," |    |    |-- Naval Military History: long (nullable = true)\n"," |    |    |-- New England US Travel Books: long (nullable = true)\n"," |    |    |-- Notebooks & Writing Pads: long (nullable = true)\n"," |    |    |-- Oboe Songbooks: long (nullable = true)\n"," |    |    |-- Office & School Supplies: long (nullable = true)\n"," |    |    |-- Office Equipment & Supplies: long (nullable = true)\n"," |    |    |-- Office Products: long (nullable = true)\n"," |    |    |-- Online Internet Searching: long (nullable = true)\n"," |    |    |-- Opera & Classical Songbooks: long (nullable = true)\n"," |    |    |-- Other RPGs: long (nullable = true)\n"," |    |    |-- Outdoor Statues: long (nullable = true)\n"," |    |    |-- Outdoors & Nature Reference: long (nullable = true)\n"," |    |    |-- Pacific Rim Cooking, Food & Wine: long (nullable = true)\n"," |    |    |-- Pacific West United States Travel Books: long (nullable = true)\n"," |    |    |-- Pain Management: long (nullable = true)\n"," |    |    |-- Paper & Printable Media: long (nullable = true)\n"," |    |    |-- Paper Craft: long (nullable = true)\n"," |    |    |-- Paper Craft Supplies: long (nullable = true)\n"," |    |    |-- Paranormal Romance: long (nullable = true)\n"," |    |    |-- Parenting: long (nullable = true)\n"," |    |    |-- Paris Travel Guides: long (nullable = true)\n"," |    |    |-- Parody: long (nullable = true)\n"," |    |    |-- Pathfinder Game: long (nullable = true)\n"," |    |    |-- Patio, Lawn & Garden: long (nullable = true)\n"," |    |    |-- Pen & Ink Drawing: long (nullable = true)\n"," |    |    |-- Pencil Drawing: long (nullable = true)\n"," |    |    |-- Percussion Songbooks: long (nullable = true)\n"," |    |    |-- Personal Finance: long (nullable = true)\n"," |    |    |-- Personal Organizers: long (nullable = true)\n"," |    |    |-- Personal Time Management: long (nullable = true)\n"," |    |    |-- Personal Transformation Self-Help: long (nullable = true)\n"," |    |    |-- Pet Food & Nutrition: long (nullable = true)\n"," |    |    |-- Photography & Video: long (nullable = true)\n"," |    |    |-- Photography Calendars: long (nullable = true)\n"," |    |    |-- Physics: long (nullable = true)\n"," |    |    |-- Piano & Keyboards: long (nullable = true)\n"," |    |    |-- Piano Songbooks: long (nullable = true)\n"," |    |    |-- Piano, Vocal & Guitar Songbooks: long (nullable = true)\n"," |    |    |-- Poetry Anthologies: long (nullable = true)\n"," |    |    |-- Political Fiction: long (nullable = true)\n"," |    |    |-- Popular & Elementary Arithmetic: long (nullable = true)\n"," |    |    |-- Popular Culture: long (nullable = true)\n"," |    |    |-- Popular Music: long (nullable = true)\n"," |    |    |-- Professional Test Guides: long (nullable = true)\n"," |    |    |-- Psychic Mysteries: long (nullable = true)\n"," |    |    |-- Psychological Thrillers: long (nullable = true)\n"," |    |    |-- Public Administration Law: long (nullable = true)\n"," |    |    |-- Puns & Wordplay: long (nullable = true)\n"," |    |    |-- Puzzle & Game Reference: long (nullable = true)\n"," |    |    |-- Puzzles: long (nullable = true)\n"," |    |    |-- Puzzles & Games: long (nullable = true)\n"," |    |    |-- Quilts & Quilting: long (nullable = true)\n"," |    |    |-- Railroad Travel Reference: long (nullable = true)\n"," |    |    |-- Reading & Phonics Teaching Materials: long (nullable = true)\n"," |    |    |-- Reading & Writing Materials: long (nullable = true)\n"," |    |    |-- Reading Skills Reference: long (nullable = true)\n"," |    |    |-- Recorder Songbooks: long (nullable = true)\n"," |    |    |-- Recorders: long (nullable = true)\n"," |    |    |-- Reference: long (nullable = true)\n"," |    |    |-- Regional & International Cooking & Wine: long (nullable = true)\n"," |    |    |-- Religion & Spirituality: long (nullable = true)\n"," |    |    |-- Religious & Inspirational Coloring Books for Grown-Ups: long (nullable = true)\n"," |    |    |-- Rock Music: long (nullable = true)\n"," |    |    |-- Rome Travel Guides: long (nullable = true)\n"," |    |    |-- Rubber Stamping: long (nullable = true)\n"," |    |    |-- Saudi Arabia Travel Guides: long (nullable = true)\n"," |    |    |-- Savannah Georgia Travel Books: long (nullable = true)\n"," |    |    |-- Saxophone Songbooks: long (nullable = true)\n"," |    |    |-- Saxophones: long (nullable = true)\n"," |    |    |-- Schools & Teaching: long (nullable = true)\n"," |    |    |-- Science & Technology Teaching Materials: long (nullable = true)\n"," |    |    |-- Science Fiction & Fantasy: long (nullable = true)\n"," |    |    |-- Science Fiction Adventures: long (nullable = true)\n"," |    |    |-- Science Fiction Graphic Novels: long (nullable = true)\n"," |    |    |-- Science Fiction Manga: long (nullable = true)\n"," |    |    |-- Scrabble: long (nullable = true)\n"," |    |    |-- Scrapbooking: long (nullable = true)\n"," |    |    |-- Scuba Diving: long (nullable = true)\n"," |    |    |-- Search Engine Optimization: long (nullable = true)\n"," |    |    |-- Self-Help: long (nullable = true)\n"," |    |    |-- Sewing Patterns & Templates: long (nullable = true)\n"," |    |    |-- Sexual Health: long (nullable = true)\n"," |    |    |-- Sheet Music Folders: long (nullable = true)\n"," |    |    |-- Sketchbooks & Notebooks: long (nullable = true)\n"," |    |    |-- Slow Cooker Recipes: long (nullable = true)\n"," |    |    |-- Small Business Bookkeeping: long (nullable = true)\n"," |    |    |-- Snow Skiing: long (nullable = true)\n"," |    |    |-- Soap Making: long (nullable = true)\n"," |    |    |-- Soccer: long (nullable = true)\n"," |    |    |-- Social Sciences: long (nullable = true)\n"," |    |    |-- Songbooks: long (nullable = true)\n"," |    |    |-- Songwriting: long (nullable = true)\n"," |    |    |-- South Atlantic United States Travel Books: long (nullable = true)\n"," |    |    |-- Space Operas: long (nullable = true)\n"," |    |    |-- Special Education: long (nullable = true)\n"," |    |    |-- Specific Artist Songbooks: long (nullable = true)\n"," |    |    |-- Sport Calendars: long (nullable = true)\n"," |    |    |-- Star Wars Game: long (nullable = true)\n"," |    |    |-- Star-Gazing: long (nullable = true)\n"," |    |    |-- Stars & Planets Field Guides: long (nullable = true)\n"," |    |    |-- Stories: long (nullable = true)\n"," |    |    |-- Strings Songbooks: long (nullable = true)\n"," |    |    |-- Study & Teaching Reference: long (nullable = true)\n"," |    |    |-- Stuffed Animal Crafts: long (nullable = true)\n"," |    |    |-- Sudoku: long (nullable = true)\n"," |    |    |-- Superhero Comics & Graphic Novels: long (nullable = true)\n"," |    |    |-- Sword & Sorcery Fantasy: long (nullable = true)\n"," |    |    |-- TV: long (nullable = true)\n"," |    |    |-- TV, Movie & Game Tie-In Fiction: long (nullable = true)\n"," |    |    |-- Tarot: long (nullable = true)\n"," |    |    |-- Teaching Materials: long (nullable = true)\n"," |    |    |-- Technology Safety & Health: long (nullable = true)\n"," |    |    |-- Teen & Young Adult Books: long (nullable = true)\n"," |    |    |-- Teen & Young Adult Crafts: long (nullable = true)\n"," |    |    |-- Teen & Young Adult Dark Fantasy: long (nullable = true)\n"," |    |    |-- Teen & Young Adult Epic Fantasy: long (nullable = true)\n"," |    |    |-- Teen & Young Adult Fantasy Action & Adventure: long (nullable = true)\n"," |    |    |-- Teen & Young Adult Games & Activities: long (nullable = true)\n"," |    |    |-- Teen & Young Adult Language Arts Books: long (nullable = true)\n"," |    |    |-- Test Flash Cards: long (nullable = true)\n"," |    |    |-- Test Prep & Study Guides: long (nullable = true)\n"," |    |    |-- Textbooks: long (nullable = true)\n"," |    |    |-- Tires: long (nullable = true)\n"," |    |    |-- Torts Law: long (nullable = true)\n"," |    |    |-- Toy Making: long (nullable = true)\n"," |    |    |-- Toys & Games: long (nullable = true)\n"," |    |    |-- Travel: long (nullable = true)\n"," |    |    |-- Travel & Scenery Calendars: long (nullable = true)\n"," |    |    |-- Travel Games: long (nullable = true)\n"," |    |    |-- Travel Language Phrasebooks: long (nullable = true)\n"," |    |    |-- Trivia: long (nullable = true)\n"," |    |    |-- Trombone Songbooks: long (nullable = true)\n"," |    |    |-- Trombones: long (nullable = true)\n"," |    |    |-- Trumpet & Cornet Songbooks: long (nullable = true)\n"," |    |    |-- U.S. State & Local History: long (nullable = true)\n"," |    |    |-- Ukuleles: long (nullable = true)\n"," |    |    |-- Unique Finds: long (nullable = true)\n"," |    |    |-- Unique Toys: long (nullable = true)\n"," |    |    |-- United States Atlases & Maps: long (nullable = true)\n"," |    |    |-- Vampire Horror: long (nullable = true)\n"," |    |    |-- Vehicle Owner's Manuals & Maintenance Guides: long (nullable = true)\n"," |    |    |-- Video & Computer Games: long (nullable = true)\n"," |    |    |-- Vietnam Travel Guides: long (nullable = true)\n"," |    |    |-- Viola Songbooks: long (nullable = true)\n"," |    |    |-- Violas: long (nullable = true)\n"," |    |    |-- Violin Songbooks: long (nullable = true)\n"," |    |    |-- Violins: long (nullable = true)\n"," |    |    |-- Vocabulary Books: long (nullable = true)\n"," |    |    |-- Vocabulary, Slang & Word Lists: long (nullable = true)\n"," |    |    |-- Vocal & Singing: long (nullable = true)\n"," |    |    |-- Vocal Songbooks: long (nullable = true)\n"," |    |    |-- Walking: long (nullable = true)\n"," |    |    |-- Wall Calendars: long (nullable = true)\n"," |    |    |-- Wall Maps: long (nullable = true)\n"," |    |    |-- Warhammer Game: long (nullable = true)\n"," |    |    |-- Weaponsmithing: long (nullable = true)\n"," |    |    |-- Web Development & Design: long (nullable = true)\n"," |    |    |-- Weight Loss Diets: long (nullable = true)\n"," |    |    |-- Weight Loss Recipes: long (nullable = true)\n"," |    |    |-- Weight Watchers Diet: long (nullable = true)\n"," |    |    |-- West Mountain United States Travel Books: long (nullable = true)\n"," |    |    |-- Winter Sports: long (nullable = true)\n"," |    |    |-- Women Sleuths: long (nullable = true)\n"," |    |    |-- Woodwinds Songbooks: long (nullable = true)\n"," |    |    |-- World War II History: long (nullable = true)\n"," |    |    |-- Wrestling: long (nullable = true)\n"," |    |    |-- Writing Skill Reference: long (nullable = true)\n"," |    |    |-- Yoga: long (nullable = true)\n"," |    |    |-- general Italy Travel Guides: long (nullable = true)\n"," |    |-- Best uses: string (nullable = true)\n"," |    |-- Bike Type: string (nullable = true)\n"," |    |-- Binding: string (nullable = true)\n"," |    |-- Binding Edge: string (nullable = true)\n"," |    |-- Biometric Security Feature: string (nullable = true)\n"," |    |-- Blade Material: string (nullable = true)\n"," |    |-- Blanket Form: string (nullable = true)\n"," |    |-- Blu ray: string (nullable = true)\n"," |    |-- Board Game: string (nullable = true)\n"," |    |-- Board book: string (nullable = true)\n"," |    |-- Body Material: string (nullable = true)\n"," |    |-- Body Shape: string (nullable = true)\n"," |    |-- Bonded Leather: string (nullable = true)\n"," |    |-- Bookmark: string (nullable = true)\n"," |    |-- Brake Style: string (nullable = true)\n"," |    |-- Brand: string (nullable = true)\n"," |    |-- Brand Name: string (nullable = true)\n"," |    |-- Breed Recommendation: string (nullable = true)\n"," |    |-- Brightness Rating: string (nullable = true)\n"," |    |-- CD ROM: string (nullable = true)\n"," |    |-- CPSIA Cautionary Statement: string (nullable = true)\n"," |    |-- Calendar: string (nullable = true)\n"," |    |-- Cap Type: string (nullable = true)\n"," |    |-- Capacitance: string (nullable = true)\n"," |    |-- Capacity: string (nullable = true)\n"," |    |-- Card Book: string (nullable = true)\n"," |    |-- Cards: string (nullable = true)\n"," |    |-- Care instructions: string (nullable = true)\n"," |    |-- Cartoon Character: string (nullable = true)\n"," |    |-- Cellular Technology: string (nullable = true)\n"," |    |-- Clarity: string (nullable = true)\n"," |    |-- Closure: string (nullable = true)\n"," |    |-- Closure Type: string (nullable = true)\n"," |    |-- Coating: string (nullable = true)\n"," |    |-- Collection Name: string (nullable = true)\n"," |    |-- Color: string (nullable = true)\n"," |    |-- Color Family: string (nullable = true)\n"," |    |-- Color Name: string (nullable = true)\n"," |    |-- Comic: string (nullable = true)\n"," |    |-- Compatible Devices: string (nullable = true)\n"," |    |-- Compatible Phone Models: string (nullable = true)\n"," |    |-- Connectivity Technology: string (nullable = true)\n"," |    |-- Connector Type: string (nullable = true)\n"," |    |-- Construction Type: string (nullable = true)\n"," |    |-- Contributor: string (nullable = true)\n"," |    |-- Controller Type: string (nullable = true)\n"," |    |-- Copyright: string (nullable = true)\n"," |    |-- Corner/Edge Style: string (nullable = true)\n"," |    |-- Country of Origin: string (nullable = true)\n"," |    |-- Country/Region of origin: string (nullable = true)\n"," |    |-- Cover Material: string (nullable = true)\n"," |    |-- Coverage: string (nullable = true)\n"," |    |-- Curtain Form: string (nullable = true)\n"," |    |-- DVD: string (nullable = true)\n"," |    |-- DVD Audio: string (nullable = true)\n"," |    |-- DVD R: string (nullable = true)\n"," |    |-- DVD ROM: string (nullable = true)\n"," |    |-- Date First Available: string (nullable = true)\n"," |    |-- Department: string (nullable = true)\n"," |    |-- Diary: string (nullable = true)\n"," |    |-- Digital: string (nullable = true)\n"," |    |-- Digital Audiobook: string (nullable = true)\n"," |    |-- Dimensions: string (nullable = true)\n"," |    |-- Diskette: string (nullable = true)\n"," |    |-- Domestic Shipping: string (nullable = true)\n"," |    |-- Edition: string (nullable = true)\n"," |    |-- Edition Number: string (nullable = true)\n"," |    |-- Educational Objective: string (nullable = true)\n"," |    |-- Electronics: string (nullable = true)\n"," |    |-- Enhanced typesetting: string (nullable = true)\n"," |    |-- Exterior: string (nullable = true)\n"," |    |-- Eyewear: string (nullable = true)\n"," |    |-- Fabric Type: string (nullable = true)\n"," |    |-- Fabric cleaning: string (nullable = true)\n"," |    |-- File Size: string (nullable = true)\n"," |    |-- File size: string (nullable = true)\n"," |    |-- Fill Material: string (nullable = true)\n"," |    |-- Film: string (nullable = true)\n"," |    |-- Finish Type: string (nullable = true)\n"," |    |-- Finish types: string (nullable = true)\n"," |    |-- Fishing Technique: string (nullable = true)\n"," |    |-- Fit Type: string (nullable = true)\n"," |    |-- Flavor: string (nullable = true)\n"," |    |-- Flexibound: string (nullable = true)\n"," |    |-- Foam Book: string (nullable = true)\n"," |    |-- Form Factor: string (nullable = true)\n"," |    |-- Format: string (nullable = true)\n"," |    |-- Fragrance Concentration: string (nullable = true)\n"," |    |-- Frame Material: string (nullable = true)\n"," |    |-- Frame Type: string (nullable = true)\n"," |    |-- Fretboard Material: string (nullable = true)\n"," |    |-- Fretboard Material Type: string (nullable = true)\n"," |    |-- Game: string (nullable = true)\n"," |    |-- Genre: string (nullable = true)\n"," |    |-- Gift: string (nullable = true)\n"," |    |-- Grade: string (nullable = true)\n"," |    |-- Grade Rating: string (nullable = true)\n"," |    |-- Grade level: string (nullable = true)\n"," |    |-- Grocery: string (nullable = true)\n"," |    |-- Guitar Bridge System: string (nullable = true)\n"," |    |-- Hand Orientation: string (nullable = true)\n"," |    |-- Handle Material: string (nullable = true)\n"," |    |-- Hardcover: string (nullable = true)\n"," |    |-- Hardcover Comic: string (nullable = true)\n"," |    |-- Hardcover spiral: string (nullable = true)\n"," |    |-- Hardware Platform: string (nullable = true)\n"," |    |-- Health and Beauty: string (nullable = true)\n"," |    |-- Home: string (nullable = true)\n"," |    |-- Human Interface Input: string (nullable = true)\n"," |    |-- ISBN 10: string (nullable = true)\n"," |    |-- ISBN 13: string (nullable = true)\n"," |    |-- Imitation Leather: string (nullable = true)\n"," |    |-- Import: string (nullable = true)\n"," |    |-- Included Components: string (nullable = true)\n"," |    |-- Ink Color: string (nullable = true)\n"," |    |-- Installation Type: string (nullable = true)\n"," |    |-- Installation Type Self-Adhesive: string (nullable = true)\n"," |    |-- Instrument: string (nullable = true)\n"," |    |-- Instrument Key: string (nullable = true)\n"," |    |-- Interactive DVD: string (nullable = true)\n"," |    |-- International Shipping: string (nullable = true)\n"," |    |-- Is Autographed: string (nullable = true)\n"," |    |-- Is Discontinued By Manufacturer: string (nullable = true)\n"," |    |-- Is Dishwasher Safe: string (nullable = true)\n"," |    |-- Is Microwaveable: string (nullable = true)\n"," |    |-- Is framed?: string (nullable = true)\n"," |    |-- Item Diameter: string (nullable = true)\n"," |    |-- Item Dimensions  LxWxH: string (nullable = true)\n"," |    |-- Item Dimensions LxWxH: string (nullable = true)\n"," |    |-- Item Display Dimensions: string (nullable = true)\n"," |    |-- Item Firmness Description: string (nullable = true)\n"," |    |-- Item Form: string (nullable = true)\n"," |    |-- Item Hardness: string (nullable = true)\n"," |    |-- Item Package Dimensions L x W x H: string (nullable = true)\n"," |    |-- Item Package Quantity: string (nullable = true)\n"," |    |-- Item Volume: string (nullable = true)\n"," |    |-- Item Weight: string (nullable = true)\n"," |    |-- Item model number: string (nullable = true)\n"," |    |-- JP Oversized: string (nullable = true)\n"," |    |-- Journal: string (nullable = true)\n"," |    |-- Kindle Edition: string (nullable = true)\n"," |    |-- Kitchen: string (nullable = true)\n"," |    |-- Label: string (nullable = true)\n"," |    |-- Lamp Type: string (nullable = true)\n"," |    |-- Language: string (nullable = true)\n"," |    |-- League: string (nullable = true)\n"," |    |-- Leather Bound: string (nullable = true)\n"," |    |-- Lexile measure: string (nullable = true)\n"," |    |-- Library Binding: string (nullable = true)\n"," |    |-- Light Source Type: string (nullable = true)\n"," |    |-- Loose Leaf: string (nullable = true)\n"," |    |-- Luminous Flux: string (nullable = true)\n"," |    |-- MP3 CD: string (nullable = true)\n"," |    |-- MP3 CD Library Binding: string (nullable = true)\n"," |    |-- Manufacturer: string (nullable = true)\n"," |    |-- Manufacturer Minimum Age (MONTHS): string (nullable = true)\n"," |    |-- Manufacturer Part Number: string (nullable = true)\n"," |    |-- Manufacturer recommended age: string (nullable = true)\n"," |    |-- Map: string (nullable = true)\n"," |    |-- Map Type: string (nullable = true)\n"," |    |-- Mass Market Paperback: string (nullable = true)\n"," |    |-- Material: string (nullable = true)\n"," |    |-- Material Aluminum, Brass: string (nullable = true)\n"," |    |-- Material Ceramic: string (nullable = true)\n"," |    |-- Material Synthetic Rubber: string (nullable = true)\n"," |    |-- Material Type: string (nullable = true)\n"," |    |-- Material Vinyl: string (nullable = true)\n"," |    |-- Maximum Voltage: string (nullable = true)\n"," |    |-- Media Format: string (nullable = true)\n"," |    |-- Memory Storage Capacity: string (nullable = true)\n"," |    |-- Microfiche: string (nullable = true)\n"," |    |-- Microfilm: string (nullable = true)\n"," |    |-- MiniDisc: string (nullable = true)\n"," |    |-- Misc: string (nullable = true)\n"," |    |-- Misc Supplies: string (nullable = true)\n"," |    |-- Model: string (nullable = true)\n"," |    |-- Model Name: string (nullable = true)\n"," |    |-- Model Year: string (nullable = true)\n"," |    |-- Mook: string (nullable = true)\n"," |    |-- Mounting Type: string (nullable = true)\n"," |    |-- Mounting Type handheld, Neck: string (nullable = true)\n"," |    |-- Music Artist: string (nullable = true)\n"," |    |-- Musical Style: string (nullable = true)\n"," |    |-- National Stock Number: string (nullable = true)\n"," |    |-- Neck Material Type: string (nullable = true)\n"," |    |-- Neck Size: string (nullable = true)\n"," |    |-- Neck Style: string (nullable = true)\n"," |    |-- Notebook: string (nullable = true)\n"," |    |-- Novelty Book: string (nullable = true)\n"," |    |-- Number Of Discs: string (nullable = true)\n"," |    |-- Number Of Items: string (nullable = true)\n"," |    |-- Number Of Pieces: string (nullable = true)\n"," |    |-- Number of Blades: string (nullable = true)\n"," |    |-- Number of Items: string (nullable = true)\n"," |    |-- Number of Keyboard Keys: string (nullable = true)\n"," |    |-- Number of Light Sources: string (nullable = true)\n"," |    |-- Number of Pieces: string (nullable = true)\n"," |    |-- Number of Players: string (nullable = true)\n"," |    |-- Number of Sets: string (nullable = true)\n"," |    |-- Number of Speeds: string (nullable = true)\n"," |    |-- Number of Strings: string (nullable = true)\n"," |    |-- Number of pieces: string (nullable = true)\n"," |    |-- Occasion: string (nullable = true)\n"," |    |-- Office Product: string (nullable = true)\n"," |    |-- Opacity: string (nullable = true)\n"," |    |-- Operating Voltage: string (nullable = true)\n"," |    |-- Operation Mode: string (nullable = true)\n"," |    |-- Orientation: string (nullable = true)\n"," |    |-- Original Release Date: string (nullable = true)\n"," |    |-- Other camera features: string (nullable = true)\n"," |    |-- Other display features: string (nullable = true)\n"," |    |-- Our Recommended age: string (nullable = true)\n"," |    |-- Package Dimensions: string (nullable = true)\n"," |    |-- Package Information: string (nullable = true)\n"," |    |-- Package Type: string (nullable = true)\n"," |    |-- Package Weight: string (nullable = true)\n"," |    |-- Packaging: string (nullable = true)\n"," |    |-- Page numbers source ISBN: string (nullable = true)\n"," |    |-- Pages: string (nullable = true)\n"," |    |-- Paint Type: string (nullable = true)\n"," |    |-- Pamphlet: string (nullable = true)\n"," |    |-- Paper Catalog: string (nullable = true)\n"," |    |-- Paper Finish: string (nullable = true)\n"," |    |-- Paper Weight: string (nullable = true)\n"," |    |-- Paperback: string (nullable = true)\n"," |    |-- Paperback Bunko: string (nullable = true)\n"," |    |-- Paperback Shinsho: string (nullable = true)\n"," |    |-- Part Number: string (nullable = true)\n"," |    |-- Pattern: string (nullable = true)\n"," |    |-- Perfect Paperback: string (nullable = true)\n"," |    |-- Perfume: string (nullable = true)\n"," |    |-- Personal Computers: string (nullable = true)\n"," |    |-- Pillow Type: string (nullable = true)\n"," |    |-- Plastic Comb: string (nullable = true)\n"," |    |-- Pocket Book: string (nullable = true)\n"," |    |-- Podcast: string (nullable = true)\n"," |    |-- Point Type: string (nullable = true)\n"," |    |-- Pole Material Type: string (nullable = true)\n"," |    |-- Pop Up: string (nullable = true)\n"," |    |-- Post-Consumer Recycled Content Percentage: string (nullable = true)\n"," |    |-- Poster: string (nullable = true)\n"," |    |-- Power Source: string (nullable = true)\n"," |    |-- Preloaded Digital Audio Player: string (nullable = true)\n"," |    |-- Pricing: string (nullable = true)\n"," |    |-- Print Magazine: string (nullable = true)\n"," |    |-- Print length: string (nullable = true)\n"," |    |-- Print on Demand: string (nullable = true)\n"," |    |-- Print on Demand Hardcover: string (nullable = true)\n"," |    |-- Print on Demand Paperback: string (nullable = true)\n"," |    |-- Printed Access Code: string (nullable = true)\n"," |    |-- Product Bundle: string (nullable = true)\n"," |    |-- Product Care Instructions: string (nullable = true)\n"," |    |-- Product Dimensions: string (nullable = true)\n"," |    |-- Proficiency Level: string (nullable = true)\n"," |    |-- Publication Date: string (nullable = true)\n"," |    |-- Publication date: string (nullable = true)\n"," |    |-- Publisher: string (nullable = true)\n"," |    |-- Puzzle: string (nullable = true)\n"," |    |-- Puzzle type: string (nullable = true)\n"," |    |-- Rag Book: string (nullable = true)\n"," |    |-- Reading Interest (Max. Age): string (nullable = true)\n"," |    |-- Reading Interest (Min. Age): string (nullable = true)\n"," |    |-- Reading age: string (nullable = true)\n"," |    |-- Recommended Uses For Product: string (nullable = true)\n"," |    |-- Recycled Content Percentage: string (nullable = true)\n"," |    |-- Refill: string (nullable = true)\n"," |    |-- Release Date: string (nullable = true)\n"," |    |-- Release date: string (nullable = true)\n"," |    |-- Reusability: string (nullable = true)\n"," |    |-- Rim Diameter: string (nullable = true)\n"," |    |-- Rim Size: string (nullable = true)\n"," |    |-- Ring bound: string (nullable = true)\n"," |    |-- Room Type: string (nullable = true)\n"," |    |-- Roughcut: string (nullable = true)\n"," |    |-- Rug Form Type: string (nullable = true)\n"," |    |-- Ruling: string (nullable = true)\n"," |    |-- Ruling Type: string (nullable = true)\n"," |    |-- Run time: string (nullable = true)\n"," |    |-- Runtime: string (nullable = true)\n"," |    |-- Scent: string (nullable = true)\n"," |    |-- School Library Binding: string (nullable = true)\n"," |    |-- Screen Reader: string (nullable = true)\n"," |    |-- Screen Size: string (nullable = true)\n"," |    |-- Screen Surface Description: string (nullable = true)\n"," |    |-- Seasons: string (nullable = true)\n"," |    |-- Series Number: string (nullable = true)\n"," |    |-- Shade Color: string (nullable = true)\n"," |    |-- Shade Material: string (nullable = true)\n"," |    |-- Shape: string (nullable = true)\n"," |    |-- Shape Irregular: string (nullable = true)\n"," |    |-- Sheet Count: string (nullable = true)\n"," |    |-- Sheet Size: string (nullable = true)\n"," |    |-- Sheet music: string (nullable = true)\n"," |    |-- Shirt form type: string (nullable = true)\n"," |    |-- Shoes: string (nullable = true)\n"," |    |-- Shooting Modes: string (nullable = true)\n"," |    |-- Simultaneous device usage: string (nullable = true)\n"," |    |-- Single Issue Magazine: string (nullable = true)\n"," |    |-- Size: string (nullable = true)\n"," |    |-- Skill Level: string (nullable = true)\n"," |    |-- Sleeve Type: string (nullable = true)\n"," |    |-- Slide: string (nullable = true)\n"," |    |-- Software: string (nullable = true)\n"," |    |-- Special Feature: string (nullable = true)\n"," |    |-- Special Features: string (nullable = true)\n"," |    |-- Special features: string (nullable = true)\n"," |    |-- Specific Uses: string (nullable = true)\n"," |    |-- Specific Uses For Product: string (nullable = true)\n"," |    |-- Specific instructions for use: string (nullable = true)\n"," |    |-- Specifications: string (nullable = true)\n"," |    |-- Spiral bound: string (nullable = true)\n"," |    |-- Sport: string (nullable = true)\n"," |    |-- Sport Type: string (nullable = true)\n"," |    |-- Sports: string (nullable = true)\n"," |    |-- Sports Apparel: string (nullable = true)\n"," |    |-- Staple Bound: string (nullable = true)\n"," |    |-- Stationery: string (nullable = true)\n"," |    |-- Sticky notes: string (nullable = true)\n"," |    |-- String Material: string (nullable = true)\n"," |    |-- Style: string (nullable = true)\n"," |    |-- Subject Keywords: string (nullable = true)\n"," |    |-- Suggested Users: string (nullable = true)\n"," |    |-- Surface Recommendation: string (nullable = true)\n"," |    |-- Surface Recommendation Cloth: string (nullable = true)\n"," |    |-- Surface Recommendation Window: string (nullable = true)\n"," |    |-- Surface treatment method: string (nullable = true)\n"," |    |-- Suspension Type: string (nullable = true)\n"," |    |-- Switch Type: string (nullable = true)\n"," |    |-- Tankobon Hardcover: string (nullable = true)\n"," |    |-- Tankobon Softcover: string (nullable = true)\n"," |    |-- Target Audience: string (nullable = true)\n"," |    |-- Target Species: string (nullable = true)\n"," |    |-- Target gender: string (nullable = true)\n"," |    |-- Team Name: string (nullable = true)\n"," |    |-- Text to Speech: string (nullable = true)\n"," |    |-- Textbook Binding: string (nullable = true)\n"," |    |-- Theme: string (nullable = true)\n"," |    |-- Thread Bound: string (nullable = true)\n"," |    |-- Thread Count: string (nullable = true)\n"," |    |-- Tick-repellent material: string (nullable = true)\n"," |    |-- Tools Hardware: string (nullable = true)\n"," |    |-- Tools Home Improvement: string (nullable = true)\n"," |    |-- Top Material: string (nullable = true)\n"," |    |-- Top Material Type: string (nullable = true)\n"," |    |-- Top Style: string (nullable = true)\n"," |    |-- Total Recycled Content Percentage: string (nullable = true)\n"," |    |-- Toy: string (nullable = true)\n"," |    |-- Transparency: string (nullable = true)\n"," |    |-- Turtleback: string (nullable = true)\n"," |    |-- Type of item: string (nullable = true)\n"," |    |-- UPC: string (nullable = true)\n"," |    |-- USB Flash Drive: string (nullable = true)\n"," |    |-- Unbound: string (nullable = true)\n"," |    |-- Unit Count: string (nullable = true)\n"," |    |-- Unknown Binding: string (nullable = true)\n"," |    |-- Usage: string (nullable = true)\n"," |    |-- VHS Tape: string (nullable = true)\n"," |    |-- Variety: string (nullable = true)\n"," |    |-- Vehicle Service Type: string (nullable = true)\n"," |    |-- Video Game: string (nullable = true)\n"," |    |-- Vinyl: string (nullable = true)\n"," |    |-- Vinyl Bound: string (nullable = true)\n"," |    |-- Voltage: string (nullable = true)\n"," |    |-- Wall Art Form: string (nullable = true)\n"," |    |-- Wall Chart: string (nullable = true)\n"," |    |-- Warranty Description: string (nullable = true)\n"," |    |-- Watch: string (nullable = true)\n"," |    |-- Water Resistance Level: string (nullable = true)\n"," |    |-- Water Resistance Technology: string (nullable = true)\n"," |    |-- Wattage: string (nullable = true)\n"," |    |-- Weave Type: string (nullable = true)\n"," |    |-- Whats in the box: string (nullable = true)\n"," |    |-- Wick Quantity: string (nullable = true)\n"," |    |-- Wireless Phone Accessory: string (nullable = true)\n"," |    |-- Wireless network technology: string (nullable = true)\n"," |    |-- With Lid: string (nullable = true)\n"," |    |-- Word Wise: string (nullable = true)\n"," |    |-- Workbook: string (nullable = true)\n"," |    |-- X Ray: string (nullable = true)\n"," |    |-- X Ray for textbooks: string (nullable = true)\n"," |    |-- Year: string (nullable = true)\n"," |-- features: array (nullable = true)\n"," |    |-- element: string (containsNull = true)\n"," |-- images: array (nullable = true)\n"," |    |-- element: struct (containsNull = true)\n"," |    |    |-- hi_res: string (nullable = true)\n"," |    |    |-- large: string (nullable = true)\n"," |    |    |-- thumb: string (nullable = true)\n"," |    |    |-- variant: string (nullable = true)\n"," |-- main_category: string (nullable = true)\n"," |-- parent_asin: string (nullable = true)\n"," |-- price: string (nullable = true)\n"," |-- rating_number: long (nullable = true)\n"," |-- store: string (nullable = true)\n"," |-- subtitle: string (nullable = true)\n"," |-- title: string (nullable = true)\n"," |-- videos: array (nullable = true)\n"," |    |-- element: struct (containsNull = true)\n"," |    |    |-- title: string (nullable = true)\n"," |    |    |-- url: string (nullable = true)\n"," |    |    |-- user_id: string (nullable = true)\n","\n"]},{"name":"stderr","output_type":"stream","text":["25/11/26 20:10:28 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"]}],"source":["spark.conf.set(\"spark.sql.caseSensitive\", \"true\")\n","\n","# books' metadata\n","df_meta = spark.read.json(meta_path)\n","df_meta.printSchema()"]},{"cell_type":"code","execution_count":9,"id":"b6d5d583","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["[Stage 3:=====================================================> (146 + 4) / 150]\r"]},{"name":"stdout","output_type":"stream","text":["root\n"," |-- asin: string (nullable = true)\n"," |-- helpful_vote: long (nullable = true)\n"," |-- images: array (nullable = true)\n"," |    |-- element: struct (containsNull = true)\n"," |    |    |-- attachment_type: string (nullable = true)\n"," |    |    |-- large_image_url: string (nullable = true)\n"," |    |    |-- medium_image_url: string (nullable = true)\n"," |    |    |-- small_image_url: string (nullable = true)\n"," |-- parent_asin: string (nullable = true)\n"," |-- rating: double (nullable = true)\n"," |-- text: string (nullable = true)\n"," |-- timestamp: long (nullable = true)\n"," |-- title: string (nullable = true)\n"," |-- user_id: string (nullable = true)\n"," |-- verified_purchase: boolean (nullable = true)\n","\n"]},{"name":"stderr","output_type":"stream","text":["\r","[Stage 3:======================================================>(149 + 1) / 150]\r","\r","                                                                                \r"]}],"source":["# books' review\n","df_reviews = spark.read.json(reviews_path)\n","df_reviews.printSchema()"]},{"cell_type":"code","execution_count":18,"id":"6f452d47","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","--- Sample Review Data ---\n","+-----+------------+------+-----------+------+-----+---------+-----+-------+-----------------+\n","| asin|helpful_vote|images|parent_asin|rating| text|timestamp|title|user_id|verified_purchase|\n","+-----+------------+------+-----------+------+-----+---------+-----+-------+-----------------+\n","|B0...|           0| [{...|      B0...|   1.0|It...|    16...|No...|  AF...|             true|\n","|05...|           1|    []|      05...|   5.0|Up...|    16...|Up...|  AF...|             true|\n","|17...|           0|    []|      17...|   5.0|I ...|    16...|Ex...|  AF...|             true|\n","|05...|           0|    []|      05...|   5.0|Up...|    16...|Up...|  AF...|            false|\n","|08...|           0| [{...|      08...|   5.0|I ...|    16...|Be...|  AF...|             true|\n","+-----+------------+------+-----------+------+-----+---------+-----+-------+-----------------+\n","only showing top 5 rows\n","\n"]}],"source":["print(\"\\n--- Sample Review Data ---\")\n","df_reviews.show(5, truncate=5)"]},{"cell_type":"code","execution_count":19,"id":"051d96d7","metadata":{},"outputs":[],"source":["from pyspark.sql import functions as F"]},{"cell_type":"code","execution_count":20,"id":"18b8375c","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["--- Calculating Top 100,000 Verified Books ---\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["Successfully cached 100000 top books.\n","+----------+\n","|      asin|\n","+----------+\n","|B00L9B7IKE|\n","|B00JO8PEN2|\n","|B006LSZECO|\n","|B00DPM7TIG|\n","|B00CNQ7HAU|\n","+----------+\n","only showing top 5 rows\n","\n"]}],"source":["# only keep top 100,000 Interaction Books with Verified Reviews\n","from pyspark.sql import functions as F\n","\n","print(\"--- Calculating Top 100,000 Verified Books ---\")\n","\n","# 1. Filter for Verified Purchases Only\n","# Based on your image, 'verified_purchase' is a boolean (true/false) column.\n","verified_df = df_reviews.filter(F.col(\"verified_purchase\") == True)\n","\n","# 2. Group by 'asin' (Book ID) and Count\n","book_counts = verified_df.groupBy(\"asin\").count()\n","\n","# 3. Order by count (descending) and take top 100,000\n","# We only select 'asin' because we don't need the count column for the Join later.\n","top_books_df = book_counts.orderBy(F.col(\"count\").desc()) \\\n","                          .limit(100000) \\\n","                          .select(\"asin\")\n","\n","# 4. CACHE THIS DATAFRAME\n","# This is critical. We will use this DF to filter BOTH the reviews and the metadata.\n","# Without caching, Spark would re-calculate the top 100k twice.\n","top_books_df.cache()\n","\n","# Trigger an action to force the computation and cache it now\n","count = top_books_df.count()\n","print(f\"Successfully cached {count} top books.\")\n","\n","top_books_df.show(5)"]},{"cell_type":"code","execution_count":21,"id":"bf04fd38","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["--- Filtering Reviews and Generating User Sequences ---\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["Number of users with valid sequences: 865397\n","\n","--- Sample Sequences (Ready for Gensim) ---\n"]},{"name":"stderr","output_type":"stream","text":["[Stage 40:==============================================>         (14 + 3) / 17]\r"]},{"name":"stdout","output_type":"stream","text":["+------------------------------------------------------------------------------------------------------------------------+\n","|sentence                                                                                                                |\n","+------------------------------------------------------------------------------------------------------------------------+\n","|B00AVMTF8U B00IJXAM80 0762782811 0425158543                                                                             |\n","|B07QYY1NN5 B07L4PL653 B07R3QYGHY                                                                                        |\n","|039958014X 0399226907 0545688868 1523501227 152350207X                                                                  |\n","|B00G3L6KQI 1538747251 1538747278 1455530441 0062311158 B00FJ3AC10 1451635834 0735213186 1416586296 0316055433 1944229450|\n","|1476812063 1423477758 0545174805                                                                                        |\n","+------------------------------------------------------------------------------------------------------------------------+\n","only showing top 5 rows\n","\n"]},{"name":"stderr","output_type":"stream","text":["25/11/26 20:34:37 ERROR TransportClient: Failed to send RPC RPC 7472540338620573548 to /10.128.0.4:44462: io.netty.channel.StacklessClosedChannelException\n","io.netty.channel.StacklessClosedChannelException: null\n","\tat io.netty.channel.AbstractChannel$AbstractUnsafe.write(Object, ChannelPromise)(Unknown Source) ~[netty-transport-4.1.100.Final.jar:4.1.100.Final]\n","25/11/26 20:34:37 WARN BlockManagerMasterEndpoint: Error trying to remove broadcast 40 from block manager BlockManagerId(35, item2vec-etl-w-0.us-central1-f.c.bookbridge-477802.internal, 41241, None)\n","java.io.IOException: Failed to send RPC RPC 7472540338620573548 to /10.128.0.4:44462: io.netty.channel.StacklessClosedChannelException\n","\tat org.apache.spark.network.client.TransportClient$RpcChannelListener.handleFailure(TransportClient.java:395) ~[spark-network-common_2.12-3.5.3.jar:3.5.3]\n","\tat org.apache.spark.network.client.TransportClient$StdChannelListener.operationComplete(TransportClient.java:372) ~[spark-network-common_2.12-3.5.3.jar:3.5.3]\n","\tat io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590) ~[netty-common-4.1.100.Final.jar:4.1.100.Final]\n","\tat io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557) ~[netty-common-4.1.100.Final.jar:4.1.100.Final]\n","\tat io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492) ~[netty-common-4.1.100.Final.jar:4.1.100.Final]\n","\tat io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636) ~[netty-common-4.1.100.Final.jar:4.1.100.Final]\n","\tat io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629) ~[netty-common-4.1.100.Final.jar:4.1.100.Final]\n","\tat io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118) ~[netty-common-4.1.100.Final.jar:4.1.100.Final]\n","\tat io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetFailure(AbstractChannel.java:999) ~[netty-transport-4.1.100.Final.jar:4.1.100.Final]\n","\tat io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:860) ~[netty-transport-4.1.100.Final.jar:4.1.100.Final]\n","\tat io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1367) ~[netty-transport-4.1.100.Final.jar:4.1.100.Final]\n","\tat io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:877) ~[netty-transport-4.1.100.Final.jar:4.1.100.Final]\n","\tat io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:863) ~[netty-transport-4.1.100.Final.jar:4.1.100.Final]\n","\tat io.netty.channel.AbstractChannelHandlerContext.write(AbstractChannelHandlerContext.java:968) ~[netty-transport-4.1.100.Final.jar:4.1.100.Final]\n","\tat io.netty.channel.AbstractChannelHandlerContext.write(AbstractChannelHandlerContext.java:856) ~[netty-transport-4.1.100.Final.jar:4.1.100.Final]\n","\tat io.netty.handler.codec.MessageToMessageEncoder.write(MessageToMessageEncoder.java:113) ~[netty-codec-4.1.100.Final.jar:4.1.100.Final]\n","\tat io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:881) ~[netty-transport-4.1.100.Final.jar:4.1.100.Final]\n","\tat io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:863) ~[netty-transport-4.1.100.Final.jar:4.1.100.Final]\n","\tat io.netty.channel.AbstractChannelHandlerContext.write(AbstractChannelHandlerContext.java:968) ~[netty-transport-4.1.100.Final.jar:4.1.100.Final]\n","\tat io.netty.channel.AbstractChannelHandlerContext.write(AbstractChannelHandlerContext.java:856) ~[netty-transport-4.1.100.Final.jar:4.1.100.Final]\n","\tat io.netty.handler.timeout.IdleStateHandler.write(IdleStateHandler.java:302) ~[netty-handler-4.1.100.Final.jar:4.1.100.Final]\n","\tat io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:879) ~[netty-transport-4.1.100.Final.jar:4.1.100.Final]\n","\tat io.netty.channel.AbstractChannelHandlerContext.invokeWriteAndFlush(AbstractChannelHandlerContext.java:940) ~[netty-transport-4.1.100.Final.jar:4.1.100.Final]\n","\tat io.netty.channel.AbstractChannelHandlerContext$WriteTask.run(AbstractChannelHandlerContext.java:1247) ~[netty-transport-4.1.100.Final.jar:4.1.100.Final]\n","\tat io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:173) ~[netty-common-4.1.100.Final.jar:4.1.100.Final]\n","\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:166) ~[netty-common-4.1.100.Final.jar:4.1.100.Final]\n","\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:470) ~[netty-common-4.1.100.Final.jar:4.1.100.Final]\n","\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:566) ~[netty-transport-4.1.100.Final.jar:4.1.100.Final]\n","\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997) ~[netty-common-4.1.100.Final.jar:4.1.100.Final]\n","\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) ~[netty-common-4.1.100.Final.jar:4.1.100.Final]\n","\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) ~[netty-common-4.1.100.Final.jar:4.1.100.Final]\n","\tat java.base/java.lang.Thread.run(Thread.java:829) [?:?]\n","Caused by: io.netty.channel.StacklessClosedChannelException\n","\tat io.netty.channel.AbstractChannel$AbstractUnsafe.write(Object, ChannelPromise)(Unknown Source) ~[netty-transport-4.1.100.Final.jar:4.1.100.Final]\n"]}],"source":["from pyspark.sql.functions import broadcast\n","\n","print(\"--- Filtering Reviews and Generating User Sequences ---\")\n","\n","# 1. Inner Join with Top Books\n","# We use 'broadcast(top_books_df)' to force a fast map-side join.\n","# This drops any review where the book is NOT in the top 100,000.\n","filtered_reviews = verified_df.join(broadcast(top_books_df), \"asin\", \"inner\")\n","\n","# 2. Group by User and Create Ordered Sequence\n","# We sort by timestamp first so the sequence represents the user's actual journey.\n","# Note: We collect the list of ASINs into a column named 'item_sequence'\n","user_sequences_df = filtered_reviews.orderBy(\"timestamp\") \\\n","    .groupBy(\"user_id\") \\\n","    .agg(F.collect_list(\"asin\").alias(\"item_sequence\"))\n","\n","# 3. Filter out users with too short history\n","# Item2Vec (Word2Vec) context window is usually 5. \n","# If a user only bought 1 or 2 items, they don't provide much context.\n","# Let's keep users with at least 3 items.\n","user_sequences_df = user_sequences_df.filter(F.size(F.col(\"item_sequence\")) >= 3)\n","\n","# 4. Convert List to Space-Separated String (For Gensim)\n","# [\"B001\", \"B002\"] -> \"B001 B002\"\n","final_training_data = user_sequences_df.select(\n","    F.concat_ws(\" \", \"item_sequence\").alias(\"sentence\")\n",")\n","\n","# --- Verification ---\n","print(f\"Number of users with valid sequences: {final_training_data.count()}\")\n","\n","print(\"\\n--- Sample Sequences (Ready for Gensim) ---\")\n","final_training_data.show(5, truncate=False)"]},{"cell_type":"code","execution_count":22,"id":"44c92dc1","metadata":{"scrolled":true},"outputs":[{"name":"stdout","output_type":"stream","text":["--- Saving Training Data to gs://book_bridge/item2vec_training_data ---\n"]},{"name":"stderr","output_type":"stream","text":["25/11/26 20:43:14 WARN YarnAllocator: Container from a bad node: container_1764180334063_0001_01_000039 on host: item2vec-etl-sw-nqrx.us-central1-f.c.bookbridge-477802.internal. Exit status: 143. Diagnostics: [2025-11-26 20:43:14.271]Container killed on request. Exit code is 143\n","[2025-11-26 20:43:14.272]Container exited with a non-zero exit code 143. \n","[2025-11-26 20:43:14.277]Killed by external signal\n",".\n","25/11/26 20:43:14 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 38 for reason Container from a bad node: container_1764180334063_0001_01_000039 on host: item2vec-etl-sw-nqrx.us-central1-f.c.bookbridge-477802.internal. Exit status: 143. Diagnostics: [2025-11-26 20:43:14.271]Container killed on request. Exit code is 143\n","[2025-11-26 20:43:14.272]Container exited with a non-zero exit code 143. \n","[2025-11-26 20:43:14.277]Killed by external signal\n",".\n","25/11/26 20:43:14 WARN YarnAllocator: Container from a bad node: container_1764180334063_0001_01_000040 on host: item2vec-etl-sw-nqrx.us-central1-f.c.bookbridge-477802.internal. Exit status: 143. Diagnostics: [2025-11-26 20:43:14.273]Container killed on request. Exit code is 143\n","[2025-11-26 20:43:14.277]Container exited with a non-zero exit code 143. \n","[2025-11-26 20:43:14.281]Killed by external signal\n",".\n","25/11/26 20:43:14 ERROR YarnScheduler: Lost executor 38 on item2vec-etl-sw-nqrx.us-central1-f.c.bookbridge-477802.internal: Container from a bad node: container_1764180334063_0001_01_000039 on host: item2vec-etl-sw-nqrx.us-central1-f.c.bookbridge-477802.internal. Exit status: 143. Diagnostics: [2025-11-26 20:43:14.271]Container killed on request. Exit code is 143\n","[2025-11-26 20:43:14.272]Container exited with a non-zero exit code 143. \n","[2025-11-26 20:43:14.277]Killed by external signal\n",".\n","25/11/26 20:43:14 WARN TaskSetManager: Lost task 89.0 in stage 48.0 (TID 2574) (item2vec-etl-sw-nqrx.us-central1-f.c.bookbridge-477802.internal executor 38): ExecutorLostFailure (executor 38 exited caused by one of the running tasks) Reason: Container from a bad node: container_1764180334063_0001_01_000039 on host: item2vec-etl-sw-nqrx.us-central1-f.c.bookbridge-477802.internal. Exit status: 143. Diagnostics: [2025-11-26 20:43:14.271]Container killed on request. Exit code is 143\n","[2025-11-26 20:43:14.272]Container exited with a non-zero exit code 143. \n","[2025-11-26 20:43:14.277]Killed by external signal\n",".\n","25/11/26 20:43:14 WARN TaskSetManager: Lost task 90.0 in stage 48.0 (TID 2575) (item2vec-etl-sw-nqrx.us-central1-f.c.bookbridge-477802.internal executor 38): ExecutorLostFailure (executor 38 exited caused by one of the running tasks) Reason: Container from a bad node: container_1764180334063_0001_01_000039 on host: item2vec-etl-sw-nqrx.us-central1-f.c.bookbridge-477802.internal. Exit status: 143. Diagnostics: [2025-11-26 20:43:14.271]Container killed on request. Exit code is 143\n","[2025-11-26 20:43:14.272]Container exited with a non-zero exit code 143. \n","[2025-11-26 20:43:14.277]Killed by external signal\n",".\n","25/11/26 20:43:14 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 39 for reason Container from a bad node: container_1764180334063_0001_01_000040 on host: item2vec-etl-sw-nqrx.us-central1-f.c.bookbridge-477802.internal. Exit status: 143. Diagnostics: [2025-11-26 20:43:14.273]Container killed on request. Exit code is 143\n","[2025-11-26 20:43:14.277]Container exited with a non-zero exit code 143. \n","[2025-11-26 20:43:14.281]Killed by external signal\n",".\n","25/11/26 20:43:14 WARN YarnAllocator: Cannot find executorId for container: container_1764180334063_0001_01_000046\n","25/11/26 20:43:14 ERROR YarnScheduler: Lost executor 39 on item2vec-etl-sw-nqrx.us-central1-f.c.bookbridge-477802.internal: Container from a bad node: container_1764180334063_0001_01_000040 on host: item2vec-etl-sw-nqrx.us-central1-f.c.bookbridge-477802.internal. Exit status: 143. Diagnostics: [2025-11-26 20:43:14.273]Container killed on request. Exit code is 143\n","[2025-11-26 20:43:14.277]Container exited with a non-zero exit code 143. \n","[2025-11-26 20:43:14.281]Killed by external signal\n",".\n","25/11/26 20:43:14 WARN TaskSetManager: Lost task 81.0 in stage 48.0 (TID 2566) (item2vec-etl-sw-nqrx.us-central1-f.c.bookbridge-477802.internal executor 39): ExecutorLostFailure (executor 39 exited caused by one of the running tasks) Reason: Container from a bad node: container_1764180334063_0001_01_000040 on host: item2vec-etl-sw-nqrx.us-central1-f.c.bookbridge-477802.internal. Exit status: 143. Diagnostics: [2025-11-26 20:43:14.273]Container killed on request. Exit code is 143\n","[2025-11-26 20:43:14.277]Container exited with a non-zero exit code 143. \n","[2025-11-26 20:43:14.281]Killed by external signal\n",".\n","25/11/26 20:43:14 WARN TaskSetManager: Lost task 84.0 in stage 48.0 (TID 2569) (item2vec-etl-sw-nqrx.us-central1-f.c.bookbridge-477802.internal executor 39): ExecutorLostFailure (executor 39 exited caused by one of the running tasks) Reason: Container from a bad node: container_1764180334063_0001_01_000040 on host: item2vec-etl-sw-nqrx.us-central1-f.c.bookbridge-477802.internal. Exit status: 143. Diagnostics: [2025-11-26 20:43:14.273]Container killed on request. Exit code is 143\n","[2025-11-26 20:43:14.277]Container exited with a non-zero exit code 143. \n","[2025-11-26 20:43:14.281]Killed by external signal\n",".\n","25/11/26 20:47:43 WARN TaskSetManager: Lost task 2.0 in stage 50.0 (TID 2641) (item2vec-etl-w-1.us-central1-f.c.bookbridge-477802.internal executor 29): FetchFailed(BlockManagerId(39, item2vec-etl-sw-nqrx.us-central1-f.c.bookbridge-477802.internal, 7337, None), shuffleId=7, mapIndex=1, mapId=2486, reduceId=166, message=\n","org.apache.spark.shuffle.FetchFailedException\n","\tat org.apache.spark.errors.SparkCoreErrors$.fetchFailedError(SparkCoreErrors.scala:437)\n","\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:1408)\n","\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:1103)\n","\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:86)\n","\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n","\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n","\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n","\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.sort_addToSorter_0$(Unknown Source)\n","\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n","\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n","\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n","\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1(ObjectHashAggregateExec.scala:92)\n","\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1$adapted(ObjectHashAggregateExec.scala:90)\n","\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:880)\n","\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:880)\n","\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n","\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n","\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n","\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n","\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n","\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n","\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n","\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n","\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n","\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: java.io.IOException: Connecting to item2vec-etl-sw-nqrx.us-central1-f.c.bookbridge-477802.internal/10.128.0.8:7337 failed in the last 4750 ms, fail this connection directly\n","\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:210)\n","\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:132)\n","\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.transferAllOutstanding(RetryingBlockTransferor.java:173)\n","\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.lambda$initiateRetry$0(RetryingBlockTransferor.java:206)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n","\t... 1 more\n","\n",")\n","25/11/26 20:47:43 WARN TaskSetManager: Lost task 8.0 in stage 50.0 (TID 2647) (item2vec-etl-w-1.us-central1-f.c.bookbridge-477802.internal executor 29): FetchFailed(BlockManagerId(38, item2vec-etl-sw-nqrx.us-central1-f.c.bookbridge-477802.internal, 7337, None), shuffleId=7, mapIndex=6, mapId=2491, reduceId=671, message=\n","org.apache.spark.shuffle.FetchFailedException\n","\tat org.apache.spark.errors.SparkCoreErrors$.fetchFailedError(SparkCoreErrors.scala:437)\n","\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:1408)\n","\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:1103)\n","\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:86)\n","\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n","\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n","\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n","\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.sort_addToSorter_0$(Unknown Source)\n","\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n","\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n","\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n","\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1(ObjectHashAggregateExec.scala:92)\n","\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1$adapted(ObjectHashAggregateExec.scala:90)\n","\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:880)\n","\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:880)\n","\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n","\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n","\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n","\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n","\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n","\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n","\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n","\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n","\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n","\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: java.io.IOException: Connecting to item2vec-etl-sw-nqrx.us-central1-f.c.bookbridge-477802.internal/10.128.0.8:7337 failed in the last 4750 ms, fail this connection directly\n","\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:210)\n","\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:132)\n","\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.transferAllOutstanding(RetryingBlockTransferor.java:173)\n","\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.lambda$initiateRetry$0(RetryingBlockTransferor.java:206)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n","\t... 1 more\n","\n",")\n"]},{"name":"stderr","output_type":"stream","text":["[Stage 50:>                                                       (0 + 11) / 13]\r","25/11/26 20:47:44 WARN TaskSetManager: Lost task 3.0 in stage 50.0 (TID 2642) (item2vec-etl-w-1.us-central1-f.c.bookbridge-477802.internal executor 40): FetchFailed(BlockManagerId(39, item2vec-etl-sw-nqrx.us-central1-f.c.bookbridge-477802.internal, 7337, None), shuffleId=7, mapIndex=1, mapId=2486, reduceId=250, message=\n","org.apache.spark.shuffle.FetchFailedException\n","\tat org.apache.spark.errors.SparkCoreErrors$.fetchFailedError(SparkCoreErrors.scala:437)\n","\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:1408)\n","\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:1103)\n","\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:86)\n","\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n","\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n","\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n","\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.sort_addToSorter_0$(Unknown Source)\n","\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n","\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n","\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n","\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1(ObjectHashAggregateExec.scala:92)\n","\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1$adapted(ObjectHashAggregateExec.scala:90)\n","\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:880)\n","\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:880)\n","\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n","\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n","\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n","\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n","\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n","\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n","\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n","\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n","\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n","\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: java.io.IOException: Connecting to item2vec-etl-sw-nqrx.us-central1-f.c.bookbridge-477802.internal/10.128.0.8:7337 failed in the last 4750 ms, fail this connection directly\n","\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:210)\n","\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:132)\n","\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.transferAllOutstanding(RetryingBlockTransferor.java:173)\n","\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.lambda$initiateRetry$0(RetryingBlockTransferor.java:206)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n","\t... 1 more\n","\n",")\n","25/11/26 20:47:44 WARN TaskSetManager: Lost task 9.0 in stage 50.0 (TID 2648) (item2vec-etl-w-1.us-central1-f.c.bookbridge-477802.internal executor 40): FetchFailed(BlockManagerId(38, item2vec-etl-sw-nqrx.us-central1-f.c.bookbridge-477802.internal, 7337, None), shuffleId=7, mapIndex=6, mapId=2491, reduceId=754, message=\n","org.apache.spark.shuffle.FetchFailedException\n","\tat org.apache.spark.errors.SparkCoreErrors$.fetchFailedError(SparkCoreErrors.scala:437)\n","\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:1408)\n","\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:1103)\n","\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:86)\n","\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n","\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n","\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n","\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.sort_addToSorter_0$(Unknown Source)\n","\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n","\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n","\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n","\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1(ObjectHashAggregateExec.scala:92)\n","\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1$adapted(ObjectHashAggregateExec.scala:90)\n","\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:880)\n","\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:880)\n","\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n","\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n","\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n","\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n","\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n","\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n","\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n","\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n","\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n","\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: java.io.IOException: Failed to connect to item2vec-etl-sw-nqrx.us-central1-f.c.bookbridge-477802.internal/10.128.0.8:7337\n","\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:294)\n","\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:214)\n","\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:132)\n","\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.transferAllOutstanding(RetryingBlockTransferor.java:173)\n","\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.lambda$initiateRetry$0(RetryingBlockTransferor.java:206)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n","\t... 1 more\n","Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: item2vec-etl-sw-nqrx.us-central1-f.c.bookbridge-477802.internal/10.128.0.8:7337\n","Caused by: java.net.ConnectException: Connection refused\n","\tat java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n","\tat java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)\n","\tat io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:337)\n","\tat io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:334)\n","\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:776)\n","\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)\n","\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)\n","\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)\n","\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)\n","\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n","\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","\n",")\n"]},{"name":"stderr","output_type":"stream","text":["\r","[Stage 50:>                                                        (0 + 9) / 13]\r","25/11/26 20:47:44 WARN TaskSetManager: Lost task 5.0 in stage 50.0 (TID 2644) (item2vec-etl-w-0.us-central1-f.c.bookbridge-477802.internal executor 44): FetchFailed(BlockManagerId(38, item2vec-etl-sw-nqrx.us-central1-f.c.bookbridge-477802.internal, 7337, None), shuffleId=7, mapIndex=6, mapId=2491, reduceId=421, message=\n","org.apache.spark.shuffle.FetchFailedException\n","\tat org.apache.spark.errors.SparkCoreErrors$.fetchFailedError(SparkCoreErrors.scala:437)\n","\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:1408)\n","\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:1103)\n","\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:86)\n","\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n","\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n","\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n","\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.sort_addToSorter_0$(Unknown Source)\n","\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n","\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n","\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n","\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1(ObjectHashAggregateExec.scala:92)\n","\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1$adapted(ObjectHashAggregateExec.scala:90)\n","\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:880)\n","\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:880)\n","\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n","\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n","\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n","\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n","\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n","\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n","\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n","\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n","\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n","\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: java.io.IOException: Connecting to item2vec-etl-sw-nqrx.us-central1-f.c.bookbridge-477802.internal/10.128.0.8:7337 failed in the last 4750 ms, fail this connection directly\n","\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:210)\n","\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:132)\n","\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.transferAllOutstanding(RetryingBlockTransferor.java:173)\n","\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.lambda$initiateRetry$0(RetryingBlockTransferor.java:206)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n","\t... 1 more\n","\n",")\n","25/11/26 20:47:44 WARN TaskSetManager: Lost task 11.0 in stage 50.0 (TID 2650) (item2vec-etl-w-0.us-central1-f.c.bookbridge-477802.internal executor 44): FetchFailed(BlockManagerId(39, item2vec-etl-sw-nqrx.us-central1-f.c.bookbridge-477802.internal, 7337, None), shuffleId=7, mapIndex=1, mapId=2486, reduceId=915, message=\n","org.apache.spark.shuffle.FetchFailedException\n","\tat org.apache.spark.errors.SparkCoreErrors$.fetchFailedError(SparkCoreErrors.scala:437)\n","\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:1408)\n","\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:1103)\n","\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:86)\n","\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n","\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n","\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n","\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.sort_addToSorter_0$(Unknown Source)\n","\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n","\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n","\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n","\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1(ObjectHashAggregateExec.scala:92)\n","\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1$adapted(ObjectHashAggregateExec.scala:90)\n","\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:880)\n","\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:880)\n","\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n","\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n","\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n","\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n","\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n","\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n","\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n","\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n","\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n","\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: java.io.IOException: Connecting to item2vec-etl-sw-nqrx.us-central1-f.c.bookbridge-477802.internal/10.128.0.8:7337 failed in the last 4750 ms, fail this connection directly\n","\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:210)\n","\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:132)\n","\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.transferAllOutstanding(RetryingBlockTransferor.java:173)\n","\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.lambda$initiateRetry$0(RetryingBlockTransferor.java:206)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n","\t... 1 more\n","\n",")\n","25/11/26 20:47:44 WARN TaskSetManager: Lost task 4.0 in stage 50.0 (TID 2643) (item2vec-etl-w-0.us-central1-f.c.bookbridge-477802.internal executor 42): FetchFailed(BlockManagerId(39, item2vec-etl-sw-nqrx.us-central1-f.c.bookbridge-477802.internal, 7337, None), shuffleId=7, mapIndex=1, mapId=2486, reduceId=336, message=\n","org.apache.spark.shuffle.FetchFailedException\n","\tat org.apache.spark.errors.SparkCoreErrors$.fetchFailedError(SparkCoreErrors.scala:437)\n","\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:1408)\n","\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:1103)\n","\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:86)\n","\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n","\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n","\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n","\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.sort_addToSorter_0$(Unknown Source)\n","\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n","\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n","\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n","\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1(ObjectHashAggregateExec.scala:92)\n","\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1$adapted(ObjectHashAggregateExec.scala:90)\n","\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:880)\n","\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:880)\n","\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n","\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n","\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n","\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n","\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n","\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n","\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n","\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n","\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n","\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: java.io.IOException: Connecting to item2vec-etl-sw-nqrx.us-central1-f.c.bookbridge-477802.internal/10.128.0.8:7337 failed in the last 4750 ms, fail this connection directly\n","\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:210)\n","\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:132)\n","\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.transferAllOutstanding(RetryingBlockTransferor.java:173)\n","\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.lambda$initiateRetry$0(RetryingBlockTransferor.java:206)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n","\t... 1 more\n","\n",")\n","25/11/26 20:47:44 WARN TaskSetManager: Lost task 10.0 in stage 50.0 (TID 2649) (item2vec-etl-w-0.us-central1-f.c.bookbridge-477802.internal executor 42): FetchFailed(BlockManagerId(39, item2vec-etl-sw-nqrx.us-central1-f.c.bookbridge-477802.internal, 7337, None), shuffleId=7, mapIndex=1, mapId=2486, reduceId=835, message=\n","org.apache.spark.shuffle.FetchFailedException\n","\tat org.apache.spark.errors.SparkCoreErrors$.fetchFailedError(SparkCoreErrors.scala:437)\n","\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:1408)\n","\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:1103)\n","\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:86)\n","\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n","\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n","\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n","\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.sort_addToSorter_0$(Unknown Source)\n","\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n","\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n","\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n","\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1(ObjectHashAggregateExec.scala:92)\n","\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1$adapted(ObjectHashAggregateExec.scala:90)\n","\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:880)\n","\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:880)\n","\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n","\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n","\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n","\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n","\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n","\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n","\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n","\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n","\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n","\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: java.io.IOException: Connecting to item2vec-etl-sw-nqrx.us-central1-f.c.bookbridge-477802.internal/10.128.0.8:7337 failed in the last 4750 ms, fail this connection directly\n","\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:210)\n","\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:132)\n","\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.transferAllOutstanding(RetryingBlockTransferor.java:173)\n","\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.lambda$initiateRetry$0(RetryingBlockTransferor.java:206)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n","\t... 1 more\n","\n",")\n","\r","[Stage 49:>                (0 + 7) / 18][Stage 50:>                (0 + 5) / 13]\r","25/11/26 20:47:44 WARN TaskSetManager: Lost task 1.0 in stage 50.0 (TID 2640) (item2vec-etl-sw-6bxj.us-central1-f.c.bookbridge-477802.internal executor 43): FetchFailed(BlockManagerId(38, item2vec-etl-sw-nqrx.us-central1-f.c.bookbridge-477802.internal, 7337, None), shuffleId=7, mapIndex=6, mapId=2491, reduceId=82, message=\n","org.apache.spark.shuffle.FetchFailedException\n","\tat org.apache.spark.errors.SparkCoreErrors$.fetchFailedError(SparkCoreErrors.scala:437)\n","\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:1408)\n","\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:1103)\n","\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:86)\n","\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n","\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n","\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n","\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.sort_addToSorter_0$(Unknown Source)\n","\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n","\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n","\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n","\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1(ObjectHashAggregateExec.scala:92)\n","\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1$adapted(ObjectHashAggregateExec.scala:90)\n","\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:880)\n","\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:880)\n","\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n","\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n","\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n","\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n","\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n","\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n","\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n","\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n","\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n","\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: java.io.IOException: Connecting to item2vec-etl-sw-nqrx.us-central1-f.c.bookbridge-477802.internal/10.128.0.8:7337 failed in the last 4750 ms, fail this connection directly\n","\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:210)\n","\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:132)\n","\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.transferAllOutstanding(RetryingBlockTransferor.java:173)\n","\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.lambda$initiateRetry$0(RetryingBlockTransferor.java:206)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n","\t... 1 more\n","\n",")\n","25/11/26 20:47:44 WARN TaskSetManager: Lost task 7.0 in stage 50.0 (TID 2646) (item2vec-etl-sw-6bxj.us-central1-f.c.bookbridge-477802.internal executor 43): FetchFailed(BlockManagerId(39, item2vec-etl-sw-nqrx.us-central1-f.c.bookbridge-477802.internal, 7337, None), shuffleId=7, mapIndex=1, mapId=2486, reduceId=587, message=\n","org.apache.spark.shuffle.FetchFailedException\n","\tat org.apache.spark.errors.SparkCoreErrors$.fetchFailedError(SparkCoreErrors.scala:437)\n","\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:1408)\n","\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:1103)\n","\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:86)\n","\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n","\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n","\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n","\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.sort_addToSorter_0$(Unknown Source)\n","\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n","\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n","\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n","\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1(ObjectHashAggregateExec.scala:92)\n","\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1$adapted(ObjectHashAggregateExec.scala:90)\n","\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:880)\n","\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:880)\n","\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n","\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n","\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n","\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n","\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n","\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n","\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n","\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n","\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n","\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: java.io.IOException: Connecting to item2vec-etl-sw-nqrx.us-central1-f.c.bookbridge-477802.internal/10.128.0.8:7337 failed in the last 4750 ms, fail this connection directly\n","\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:210)\n","\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:132)\n","\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.transferAllOutstanding(RetryingBlockTransferor.java:173)\n","\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.lambda$initiateRetry$0(RetryingBlockTransferor.java:206)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n","\t... 1 more\n","\n",")\n"]},{"name":"stderr","output_type":"stream","text":["25/11/26 20:47:58 WARN TaskSetManager: Lost task 12.0 in stage 50.0 (TID 2651) (item2vec-etl-w-1.us-central1-f.c.bookbridge-477802.internal executor 29): FetchFailed(BlockManagerId(39, item2vec-etl-sw-nqrx.us-central1-f.c.bookbridge-477802.internal, 7337, None), shuffleId=7, mapIndex=1, mapId=2486, reduceId=994, message=\n","org.apache.spark.shuffle.FetchFailedException\n","\tat org.apache.spark.errors.SparkCoreErrors$.fetchFailedError(SparkCoreErrors.scala:437)\n","\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:1408)\n","\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:1103)\n","\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:86)\n","\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n","\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n","\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n","\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.sort_addToSorter_0$(Unknown Source)\n","\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n","\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n","\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n","\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1(ObjectHashAggregateExec.scala:92)\n","\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1$adapted(ObjectHashAggregateExec.scala:90)\n","\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:880)\n","\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:880)\n","\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n","\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n","\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n","\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n","\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n","\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n","\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n","\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n","\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n","\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: java.io.IOException: Connecting to item2vec-etl-sw-nqrx.us-central1-f.c.bookbridge-477802.internal/10.128.0.8:7337 failed in the last 4750 ms, fail this connection directly\n","\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:210)\n","\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:132)\n","\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.transferAllOutstanding(RetryingBlockTransferor.java:173)\n","\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.lambda$initiateRetry$0(RetryingBlockTransferor.java:206)\n","\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n","\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n","\t... 1 more\n","\n",")\n","                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["Training data saved successfully.\n"]},{"name":"stderr","output_type":"stream","text":["25/11/26 20:49:44 WARN TaskSetManager: Lost task 0.0 in stage 50.0 (TID 2639) (item2vec-etl-sw-6bxj.us-central1-f.c.bookbridge-477802.internal executor 41): FetchFailed(BlockManagerId(39, item2vec-etl-sw-nqrx.us-central1-f.c.bookbridge-477802.internal, 7337, None), shuffleId=7, mapIndex=1, mapId=2486, reduceId=0, message=\n","org.apache.spark.shuffle.FetchFailedException\n","\tat org.apache.spark.errors.SparkCoreErrors$.fetchFailedError(SparkCoreErrors.scala:437)\n","\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:1408)\n","\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:1103)\n","\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:86)\n","\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n","\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n","\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n","\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.sort_addToSorter_0$(Unknown Source)\n","\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n","\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n","\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n","\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1(ObjectHashAggregateExec.scala:92)\n","\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1$adapted(ObjectHashAggregateExec.scala:90)\n","\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:880)\n","\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:880)\n","\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n","\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n","\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n","\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n","\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n","\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n","\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n","\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n","\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n","\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.network.client.ChunkFetchFailureException: Failure while fetching StreamChunkId[streamId=2136718562000,chunkIndex=0]: java.lang.RuntimeException: Executor is not registered (appId=application_1764180334063_0001, execId=39)\n","\tat org.apache.spark.network.shuffle.ExternalShuffleBlockResolver.getContinuousBlocksData(ExternalShuffleBlockResolver.java:207)\n","\tat org.apache.spark.network.shuffle.ExternalBlockHandler$ShuffleManagedBufferIterator.next(ExternalBlockHandler.java:660)\n","\tat org.apache.spark.network.shuffle.ExternalBlockHandler$ShuffleManagedBufferIterator.next(ExternalBlockHandler.java:613)\n","\tat org.apache.spark.network.server.OneForOneStreamManager.getChunk(OneForOneStreamManager.java:101)\n","\tat org.apache.spark.network.server.ChunkFetchRequestHandler.processFetchRequest(ChunkFetchRequestHandler.java:103)\n","\tat org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:107)\n","\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)\n","\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)\n","\tat org.sparkproject.io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)\n","\tat org.sparkproject.io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:442)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)\n","\tat org.sparkproject.io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)\n","\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)\n","\tat org.sparkproject.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\n","\tat org.sparkproject.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\n","\tat org.sparkproject.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)\n","\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)\n","\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)\n","\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)\n","\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)\n","\tat org.sparkproject.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)\n","\tat org.sparkproject.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n","\tat org.sparkproject.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","\n","\tat org.apache.spark.network.client.TransportResponseHandler.handle(TransportResponseHandler.java:183)\n","\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:142)\n","\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)\n","\tat io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)\n","\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)\n","\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\n","\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)\n","\tat io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)\n","\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:442)\n","\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\n","\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)\n","\tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)\n","\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)\n","\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\n","\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)\n","\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)\n","\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)\n","\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\n","\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)\n","\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\n","\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)\n","\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\n","\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\n","\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)\n","\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)\n","\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)\n","\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)\n","\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)\n","\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)\n","\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n","\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n","\t... 1 more\n","\n",")\n","25/11/26 20:49:44 WARN TaskSetManager: Lost task 6.0 in stage 50.0 (TID 2645) (item2vec-etl-sw-6bxj.us-central1-f.c.bookbridge-477802.internal executor 41): FetchFailed(BlockManagerId(38, item2vec-etl-sw-nqrx.us-central1-f.c.bookbridge-477802.internal, 7337, None), shuffleId=7, mapIndex=6, mapId=2491, reduceId=504, message=\n","org.apache.spark.shuffle.FetchFailedException\n","\tat org.apache.spark.errors.SparkCoreErrors$.fetchFailedError(SparkCoreErrors.scala:437)\n","\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:1408)\n","\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:1103)\n","\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:86)\n","\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n","\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n","\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n","\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.sort_addToSorter_0$(Unknown Source)\n","\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n","\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n","\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n","\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1(ObjectHashAggregateExec.scala:92)\n","\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1$adapted(ObjectHashAggregateExec.scala:90)\n","\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:880)\n","\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:880)\n","\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n","\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n","\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n","\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n","\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n","\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n","\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n","\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n","\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n","\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: org.apache.spark.network.client.ChunkFetchFailureException: Failure while fetching StreamChunkId[streamId=2136718562001,chunkIndex=0]: java.lang.RuntimeException: Executor is not registered (appId=application_1764180334063_0001, execId=38)\n","\tat org.apache.spark.network.shuffle.ExternalShuffleBlockResolver.getContinuousBlocksData(ExternalShuffleBlockResolver.java:207)\n","\tat org.apache.spark.network.shuffle.ExternalBlockHandler$ShuffleManagedBufferIterator.next(ExternalBlockHandler.java:660)\n","\tat org.apache.spark.network.shuffle.ExternalBlockHandler$ShuffleManagedBufferIterator.next(ExternalBlockHandler.java:613)\n","\tat org.apache.spark.network.server.OneForOneStreamManager.getChunk(OneForOneStreamManager.java:101)\n","\tat org.apache.spark.network.server.ChunkFetchRequestHandler.processFetchRequest(ChunkFetchRequestHandler.java:103)\n","\tat org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:107)\n","\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)\n","\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)\n","\tat org.sparkproject.io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)\n","\tat org.sparkproject.io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:442)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)\n","\tat org.sparkproject.io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)\n","\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)\n","\tat org.sparkproject.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\n","\tat org.sparkproject.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\n","\tat org.sparkproject.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)\n","\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)\n","\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)\n","\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)\n","\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)\n","\tat org.sparkproject.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)\n","\tat org.sparkproject.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n","\tat org.sparkproject.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","\n","\tat org.apache.spark.network.client.TransportResponseHandler.handle(TransportResponseHandler.java:183)\n","\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:142)\n","\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)\n","\tat io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)\n","\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)\n","\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\n","\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)\n","\tat io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)\n","\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:442)\n","\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\n","\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)\n","\tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)\n","\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)\n","\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\n","\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)\n","\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)\n","\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)\n","\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\n","\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)\n","\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\n","\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)\n","\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\n","\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\n","\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)\n","\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)\n","\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)\n","\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)\n","\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)\n","\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)\n","\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n","\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n","\t... 1 more\n","\n",")\n"]}],"source":["# Output path for training data\n","OUTPUT_TRAIN_PATH = f\"gs://{BUCKET_NAME}/item2vec_training_data\"\n","\n","print(f\"--- Saving Training Data to {OUTPUT_TRAIN_PATH} ---\")\n","\n","# Save as compressed text\n","# Note: We do NOT use coalesce(1) here because the data is still huge and gensim can reading multi-file streamingly,\n","final_training_data.write.mode(\"overwrite\").text(OUTPUT_TRAIN_PATH, compression=\"gzip\")\n","\n","print(\"Training data saved successfully.\")"]},{"cell_type":"code","execution_count":24,"id":"b2511a8d","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["--- Filtering and Saving Metadata for API ---\n"]},{"name":"stderr","output_type":"stream","text":["\r","                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["+----------+--------------------+-------------+--------------+-------------+--------------------+--------------------+-----------------+--------------------+\n","|      asin|               title|main_category|average_rating|rating_number|         description|          categories|      author_name|       primary_image|\n","+----------+--------------------+-------------+--------------+-------------+--------------------+--------------------+-----------------+--------------------+\n","|0316185361|Service: A Navy S...|        Books|           4.7|         3421|[Review, Praise f...|[Books, Biographi...|  Marcus Luttrell|{NULL, https://m....|\n","|1680450263|Make: Electronics...|        Books|           4.7|         1366|[From the Author,...|[Books, Engineeri...|    Charles Platt|{NULL, https://m....|\n","|1932225323|Four Centuries of...|        Books|           4.8|          133|[About the Author...|[Books, Education...|     David Barton|{NULL, https://m....|\n","|B003P2WETK|Inspector Imanish...| Buy a Kindle|           4.0|         1138|[Review, Praise f...|[Books, Mystery, ...| Seicho Matsumoto|{NULL, https://m....|\n","|0937295760|Kirsten: An Ameri...|        Books|           4.5|          123|                  []|[Books, Boxed Set...|Janet Beeler Shaw|{NULL, https://m....|\n","+----------+--------------------+-------------+--------------+-------------+--------------------+--------------------+-----------------+--------------------+\n","only showing top 5 rows\n","\n"]}],"source":["print(\"--- Filtering and Saving Metadata for API ---\")\n","\n","# Select and Clean Columns\n","# We apply the transformation BEFORE the join to save memory\n","clean_meta_df = df_meta.select(\n","    F.col(\"parent_asin\").alias(\"asin\"),  # Rename to match your reviews key\n","    F.col(\"title\"),\n","    F.col(\"main_category\"),\n","    F.col(\"average_rating\"),\n","    F.col(\"rating_number\"),\n","    F.col(\"description\"),         # This is an array of strings\n","    F.col(\"categories\"),          # This is an array of strings\n","    F.col(\"author.name\").alias(\"author_name\"),  # Flatten nested struct\n","    F.col(\"images\").getItem(0).alias(\"primary_image\") # Get only the first image object\n",")\n","\n","# Filter using the cached Top 100k Books\n","# Note: We rely on 'asin' matching between reviews and metadata.\n","filtered_meta = clean_meta_df.join(broadcast(top_books_df), \"asin\", \"inner\")\n","\n","\n","# Handle Missing Titles/Images (Optional Cleaning)\n","# It's good practice for an API to replace nulls with default values\n","filtered_meta = filtered_meta.na.fill({\n","    \"title\": \"Unknown Title\",\n","    \"main_category\": \"Uncategorized\",\n","    \"author_name\": \"Unknown Author\"\n","})\n","\n","# preview cleaned meta data\n","filtered_meta.show(5)"]},{"cell_type":"code","execution_count":25,"id":"1278dc62","metadata":{"scrolled":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Saving cleaned metadata to gs://book_bridge/filtered_metadata...\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["--- Metadata Processing Complete ---\n"]}],"source":["# Save as a SINGLE JSONL file\n","OUTPUT_META_PATH = f\"gs://{BUCKET_NAME}/filtered_metadata\"\n","\n","print(f\"Saving cleaned metadata to {OUTPUT_META_PATH}...\")\n","\n","# coalesce(1) puts everything in one file for easy loading in your API\n","filtered_meta.coalesce(1).write.mode(\"overwrite\").json(OUTPUT_META_PATH)\n","\n","print(\"--- Metadata Processing Complete ---\")"]},{"cell_type":"code","execution_count":null,"id":"efd9f95c","metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.8"}},"nbformat":4,"nbformat_minor":5}